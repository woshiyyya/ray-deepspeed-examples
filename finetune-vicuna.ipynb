{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/mnt/cluster_storage/vicuna-13b/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 16\n",
    "BATCH_SIZE_PER_WORKER = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 23:38:59,574\tINFO worker.py:1426 -- Connecting to existing Ray cluster at address: 10.0.104.253:6379...\n",
      "2023-06-23 23:38:59,639\tINFO worker.py:1607 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-vzyh3916u4zwmf1es6fazmbrgm.i.anyscaleuserdata-staging.com \u001b[39m\u001b[22m\n",
      "2023-06-23 23:38:59,642\tINFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_71de2ab32420c3943f2aef46eb349233.zip' (0.05MiB) to Ray cluster...\n",
      "2023-06-23 23:38:59,643\tINFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_71de2ab32420c3943f2aef46eb349233.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(download_vicuna_13b pid=5790, ip=10.0.123.219)\u001b[0m 3: Finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "import os\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "def download_vicuna_13b(rank):\n",
    "    if not os.path.exists(\"/tmp/vicuna-13b\"):\n",
    "        print(f\"{rank}: Downloading vicuna model\")\n",
    "        os.system(\"aws s3 sync s3://large-dl-models-mirror/restricted/models--lmsys--vicuna-13b-delta-v1.1/main-safetensors/ /tmp/vicuna-13b >NUL 2>&1\")\n",
    "    print(f\"{rank}: Finished\")\n",
    "    return True\n",
    "\n",
    "\n",
    "tasks = [download_vicuna_13b.remote(i) for i in range(NUM_WORKERS)]\n",
    "ray.get(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "config = AutoConfig.from_pretrained(\"/tmp/vicuna-13b\")\n",
    "HIDDEN_SIZE = config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-23 23:39:03,430] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cosmos_qa (/home/ray/.cache/huggingface/datasets/cosmos_qa/default/0.1.0/3e18538cbfdb2c04189b16642715f0f6da3e97ed5df0aadcec3641245b2cf157)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5014fe67ac1c4ee0aff62f2b7349723c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 23:39:06,191\tWARNING dataset.py:249 -- \u001b[33mImportant: Ray Data requires schemas for all datasets in Ray 2.5. This means that standalone Python objects are no longer supported. In addition, the default batch format is fixed to NumPy. To revert to legacy behavior temporarily, set the environment variable RAY_DATA_STRICT_MODE=0 on all cluster processes.\n",
      "\n",
      "Learn more here: https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.air.config import CheckpointConfig, RunConfig, ScalingConfig\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from ray.data.preprocessors import BatchMapper, Chain\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from deepspeed.ops.adam import DeepSpeedCPUAdam\n",
    "\n",
    "# MODEL_NAME = \"lmsys/vicuna-13b-delta-v1.1\"\n",
    "# MODEL_NAME = \"EleutherAI/gpt-j-6B\"\n",
    "# MODEL_NAME = \"tiiuae/falcon-7b\"\n",
    "# MODEL_NAME = \"mosaicml/mpt-7b\"\n",
    "MODEL_NAME = \"/tmp/vicuna-13b\"\n",
    "\n",
    "# hf_dataset = load_dataset(\"xtreme\", \"MLQA.en.en\")\n",
    "hf_dataset = load_dataset(\"cosmos_qa\")[\"train\"]\n",
    "ray_dataset = ray.data.from_huggingface(hf_dataset).limit(8000)\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Based on the context, the answer to the question would be: {answer} </s>\n",
    "\"\"\"\n",
    "\n",
    "def fill_prompt(batch):\n",
    "    batch[\"input_sentence\"] = batch.apply(\n",
    "        lambda row: PROMPT_TEMPLATE.format(\n",
    "            context=row[\"context\"],\n",
    "            question=row[\"question\"],\n",
    "            answer=row[\"answer0\"]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    return batch[[\"input_sentence\"]]\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    ret = tokenizer(\n",
    "        list(batch[\"input_sentence\"]),\n",
    "        truncation=True,\n",
    "        max_length=180,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    ret[\"labels\"] = ret[\"input_ids\"].copy()\n",
    "    return dict(ret)\n",
    "\n",
    "\n",
    "prompt_mapper = BatchMapper(fill_prompt, batch_format=\"pandas\")\n",
    "tokenize_mapper = BatchMapper(tokenize, batch_format=\"pandas\")\n",
    "preprocessor = Chain(prompt_mapper, tokenize_mapper)\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import transformers.deepspeed\n",
    "\n",
    "\n",
    "class ZeRO3Config:\n",
    "    def __init__(self, pl_module):\n",
    "        self.config = pl_module.trainer.strategy.config\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def is_zero3(self) -> bool:\n",
    "        return True\n",
    "\n",
    "\n",
    "def enable_transformers_pretrained_deepspeed_sharding(\n",
    "    pl_module: \"pl.LightningModule\",\n",
    ") -> None:\n",
    "    transformers.deepspeed._hf_deepspeed_config_weak_ref = ZeRO3Config(pl_module)\n",
    "\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "\n",
    "class Falcon7BModel(pl.LightningModule):\n",
    "    def __init__(self, inference=False):\n",
    "        super().__init__()\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        if inference:\n",
    "            with init_empty_weights():\n",
    "                config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "                self.model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
    "            self.model.tie_weights()\n",
    "        # else:\n",
    "        #     enable_transformers_pretrained_deepspeed_sharding(self)\n",
    "        #     self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        #     self.model.tie_weights()\n",
    "        \n",
    "    def setup(self, stage) -> None:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            print(\"Config :\", self.trainer.strategy.config)\n",
    "            enable_transformers_pretrained_deepspeed_sharding(self)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "            print(self.model)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        outputs = self.model(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"],\n",
    "        )\n",
    "        return outputs.loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        torch.cuda.empty_cache()\n",
    "        loss = self.forward(batch)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return DeepSpeedCPUAdam(self.parameters(), lr=1e-5, fp32_optimizer_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 23:39:06,846\tINFO streaming_executor.py:91 -- Executing DAG InputDataBuffer[Input] -> LimitOperator[limit=8000]\n",
      "2023-06-23 23:39:06,847\tINFO streaming_executor.py:92 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-06-23 23:39:06,847\tINFO streaming_executor.py:94 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa19055450d74c39a6bf75ceba88af78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "\n",
    "# Create a customized progress bar for LightningTrainer\n",
    "class FalconProgressBar(TQDMProgressBar):\n",
    "    def __init__(self, num_iters_per_epoch, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_iters_per_epoch = num_iters_per_epoch\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, *_):\n",
    "        super().on_train_epoch_start(trainer, *_)\n",
    "        self.train_progress_bar.reset(self.num_iters_per_epoch)\n",
    "\n",
    "\n",
    "total_batches = ray_dataset.count()\n",
    "num_iters_per_epoch = total_batches // (NUM_WORKERS * BATCH_SIZE_PER_WORKER)\n",
    "progress_bar = FalconProgressBar(num_iters_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.lightning import LightningTrainer, LightningConfigBuilder\n",
    "\n",
    "# GROUP_SIZE = 1.7e7\n",
    "GROUP_SIZE = 1e8\n",
    "\n",
    "deepspeed_configs = {\n",
    "    \"zero_allow_untested_optimizer\": True,\n",
    "    # \"fp16\": {\n",
    "    #     \"enabled\": True,\n",
    "    #     \"initial_scale_power\": 8,\n",
    "    # },\n",
    "    \"bf16\": {\n",
    "        \"enabled\": True\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        },\n",
    "        # \"offload_param\": {\n",
    "        #     \"device\": \"cpu\",\n",
    "        #     \"pin_memory\": True\n",
    "        # },\n",
    "        \"overlap_comm\": True,\n",
    "        \"contiguous_gradients\": True,\n",
    "        # \"sub_group_size\": GROUP_SIZE,\n",
    "        \"reduce_bucket_size\": HIDDEN_SIZE * HIDDEN_SIZE,\n",
    "        \"stage3_prefetch_bucket_size\": 0.9 * HIDDEN_SIZE * HIDDEN_SIZE,\n",
    "        \"stage3_param_persistence_threshold\": 10 * HIDDEN_SIZE,\n",
    "        # \"stage3_max_live_parameters\": 2e8,\n",
    "        # \"stage3_max_reuse_distance\": 2e8,\n",
    "    },\n",
    "    # \"activation_checkpointing\":{\n",
    "    #     \"partition_activations\":True,\n",
    "    #     \"cpu_checkpointing\":True,\n",
    "    # },\n",
    "    # \"autotuning\": {\"enabled\": True}\n",
    "}\n",
    "\n",
    "lightning_config = (\n",
    "    LightningConfigBuilder()\n",
    "    .module(cls=Falcon7BModel)\n",
    "    .trainer(\n",
    "        max_epochs=1, accelerator=\"gpu\", precision=\"bf16-mixed\", callbacks=[progress_bar], accumulate_grad_batches=2\n",
    "    )\n",
    "    .strategy(\n",
    "        name=\"deepspeed\",\n",
    "        # stage=3,\n",
    "        config=deepspeed_configs\n",
    "        # offload_optimizer=True,\n",
    "        # offload_parameters=True,\n",
    "        # offload_params_device=\"cpu\",\n",
    "        # offload_optimizer_device=\"cpu\",\n",
    "        # partition_activations=True,\n",
    "        # cpu_checkpointing=True,\n",
    "        # contiguous_gradients=True,\n",
    "        # reduce_bucket_size=2e8,\n",
    "        # allgather_bucket_size=\"auto\",\n",
    "        # sub_group_size=\"auto\",\n",
    "    )\n",
    "    .checkpointing(save_top_k=0, save_weights_only=True, save_last=True)\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 23:39:10,866\tWARNING base_trainer.py:201 -- The `preprocessor` arg to Trainer is deprecated. Apply preprocessor transformations ahead of time by calling `preprocessor.transform(ds)`. Support for the preprocessor arg will be dropped in a future release.\n"
     ]
    }
   ],
   "source": [
    "trainer = LightningTrainer(\n",
    "    lightning_config=lightning_config,\n",
    "    run_config=RunConfig(\n",
    "        name=\"vicuna-13b-finetune\", \n",
    "        storage_path=\"s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-test\",\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            _checkpoint_keep_all_ranks=True,\n",
    "            _checkpoint_upload_from_workers=True\n",
    "        ),\n",
    "    ),\n",
    "    scaling_config=ScalingConfig(num_workers=NUM_WORKERS, use_gpu=True, resources_per_worker={\"CPU\": 15, \"GPU\": 1}),\n",
    "    datasets={\"train\": ray_dataset},\n",
    "    datasets_iter_config={\"batch_size\": BATCH_SIZE_PER_WORKER},\n",
    "    preprocessor=preprocessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-06-24 00:59:43</td></tr>\n",
       "<tr><td>Running for: </td><td>01:20:32.10        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.3/62.1 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 241.0/256 CPUs, 16.0/16 GPUs (0.0/16.0 accelerator_type:A10G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">  step</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_d3e90_00000</td><td>TERMINATED</td><td>10.0.84.83:5998</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4757.51</td><td style=\"text-align: right;\">     1.46094</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">    62</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=5998, ip=10.0.84.83)\u001b[0m [2023-06-23 23:39:15,728] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[2m\u001b[36m(download_vicuna_13b pid=42002, ip=10.0.121.227)\u001b[0m 15: Finished\u001b[32m [repeated 15x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LightningTrainer pid=5998, ip=10.0.84.83)\u001b[0m The `preprocessor` arg to Trainer is deprecated. Apply preprocessor transformations ahead of time by calling `preprocessor.transform(ds)`. Support for the preprocessor arg will be dropped in a future release.\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=5998, ip=10.0.84.83)\u001b[0m \u001b[33mImportant: Ray Data requires schemas for all datasets in Ray 2.5. This means that standalone Python objects are no longer supported. In addition, the default batch format is fixed to NumPy. To revert to legacy behavior temporarily, set the environment variable RAY_DATA_STRICT_MODE=0 on all cluster processes.\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=5998, ip=10.0.84.83)\u001b[0m \n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=5998, ip=10.0.84.83)\u001b[0m Learn more here: https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode\u001b[0m\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=5998, ip=10.0.84.83)\u001b[0m Starting distributed worker processes: ['6053 (10.0.84.83)', '6364 (10.0.95.215)', '6074 (10.0.79.156)', '6110 (10.0.120.86)', '5898 (10.0.66.134)', '6087 (10.0.72.137)', '17232 (10.0.104.253)', '5975 (10.0.66.239)', '5881 (10.0.123.219)', '6001 (10.0.103.43)', '5941 (10.0.117.132)', '5845 (10.0.64.104)', '48011 (10.0.121.227)', '5966 (10.0.90.111)', '5970 (10.0.87.254)', '5903 (10.0.121.7)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m Setting up process group for: env:// [rank=0, world_size=16]\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=5998, ip=10.0.84.83)\u001b[0m Executing DAG InputDataBuffer[Input] -> LimitOperator[limit=8000] -> TaskPoolMapOperator[MapBatches(BatchMapper._transform_pandas)->MapBatches(BatchMapper._transform_pandas)] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=5998, ip=10.0.84.83)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=5998, ip=10.0.84.83)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e659740da534f3fb76c7cc68d5c7989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=5998, ip=10.0.84.83) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337fa3dcda91443a92def0a19c2e2f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=5998, ip=10.0.84.83) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6074, ip=10.0.79.156)\u001b[0m [2023-06-23 23:39:26,513] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6087, ip=10.0.72.137)\u001b[0m initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/16\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6087, ip=10.0.72.137)\u001b[0m Missing logger folder: /home/ray/ray_results/vicuna-13b-finetune/LightningTrainer_d3e90_00000_0_2023-06-23_23-39-11/rank_all/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6087, ip=10.0.72.137)\u001b[0m [2023-06-23 23:39:27,262] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5975, ip=10.0.66.239)\u001b[0m Config : {'zero_allow_untested_optimizer': True, 'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'overlap_comm': True, 'contiguous_gradients': True, 'reduce_bucket_size': 26214400, 'stage3_prefetch_bucket_size': 23592960.0, 'stage3_param_persistence_threshold': 51200}, 'gradient_accumulation_steps': 2, 'train_micro_batch_size_per_gpu': 1, 'gradient_clipping': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5975, ip=10.0.66.239)\u001b[0m initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/16\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5975, ip=10.0.66.239)\u001b[0m Missing logger folder: /home/ray/ray_results/vicuna-13b-finetune/LightningTrainer_d3e90_00000_0_2023-06-23_23-39-11/rank_all/lightning_logs\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.23s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [00:23<00:11, 11.95s/it]\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:31<00:00, 10.65s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [00:28<00:14, 14.13s/it]\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m LlamaForCausalLM(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m   (model): LlamaModel(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m     (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m     (layers): ModuleList(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m       (0-39): 40 x LlamaDecoderLayer(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m         (self_attn): LlamaAttention(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m           (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m           (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m           (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m           (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m           (rotary_emb): LlamaRotaryEmbedding()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m         )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m         (mlp): LlamaMLP(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m           (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m           (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m           (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m           (act_fn): SiLUActivation()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m         )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m         (input_layernorm): LlamaRMSNorm()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m         (post_attention_layernorm): LlamaRMSNorm()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m       )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m     )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m     (norm): LlamaRMSNorm()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m   )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m   (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m [2023-06-23 23:39:26,577] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5975, ip=10.0.66.239)\u001b[0m [2023-06-23 23:39:27,265] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5903, ip=10.0.121.7)\u001b[0m Config : {'zero_allow_untested_optimizer': True, 'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'overlap_comm': True, 'contiguous_gradients': True, 'reduce_bucket_size': 26214400, 'stage3_prefetch_bucket_size': 23592960.0, 'stage3_param_persistence_threshold': 51200}, 'gradient_accumulation_steps': 2, 'train_micro_batch_size_per_gpu': 1, 'gradient_clipping': 0.0}\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5941, ip=10.0.117.132)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:32<00:00, 10.68s/it]\u001b[32m [repeated 14x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m LlamaForCausalLM(\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m   (model): LlamaModel(\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m     (embed_tokens): Embedding(32000, 5120, padding_idx=0)\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m     (layers): ModuleList(\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m       (0-39): 40 x LlamaDecoderLayer(\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m         (self_attn): LlamaAttention(\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m           (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m           (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m           (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m           (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m           (rotary_emb): LlamaRotaryEmbedding()\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m )\u001b[32m [repeated 90x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m         (mlp): LlamaMLP(\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m           (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m           (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m           (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m           (act_fn): SiLUActivation()\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m         (input_layernorm): LlamaRMSNorm()\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m         (post_attention_layernorm): LlamaRMSNorm()\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m     (norm): LlamaRMSNorm()\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m   (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6074, ip=10.0.79.156)\u001b[0m Using /home/ray/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6087, ip=10.0.72.137)\u001b[0m Detected CUDA files, patching ldflags\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6087, ip=10.0.72.137)\u001b[0m Emitting ninja build file /home/ray/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6087, ip=10.0.72.137)\u001b[0m Building extension module cpu_adam...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6087, ip=10.0.72.137)\u001b[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m ninja: no work to do.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m Time to load cpu_adam op: 2.373521566390991 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:38<00:00, 12.86s/it]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5966, ip=10.0.90.111)\u001b[0m Building extension module utils...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m Time to load utils op: 0.0738983154296875 seconds\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m Parameter Offload: Total persistent parameters: 414720 in 81 params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m ninja: no work to do.\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48011, ip=10.0.121.227)\u001b[0m Time to load cpu_adam op: 2.3655214309692383 seconds\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m Time to load utils op: 0.0006968975067138672 seconds\u001b[32m [repeated 16x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m Using /home/ray/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[32m [repeated 32x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48011, ip=10.0.121.227)\u001b[0m Detected CUDA files, patching ldflags\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m Emitting ninja build file /home/ray/.cache/torch_extensions/py310_cu118/utils/build.ninja...\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48011, ip=10.0.121.227)\u001b[0m Building extension module cpu_adam...\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48011, ip=10.0.121.227)\u001b[0m Loading extension module cpu_adam...\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m Building extension module utils...\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m Loading extension module utils...\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m   | Name  | Type             | Params | Params per Device\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m ---------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m 0 | model | LlamaForCausalLM | 13.0 B | 813 M            \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m ---------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m 13.0 B    Trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m 13.0 B    Total params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m 52,063.457Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/125 [00:00<?, ?it/s])\u001b[0m \n",
      "Epoch 0:   1%|          | 1/125 [00:38<1:18:32, 38.00s/it, v_num=0, train_loss=4.120]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m Time to load utils op: 0.00030040740966796875 seconds\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Epoch 0:   2%|▏         | 2/125 [01:15<1:17:29, 37.80s/it, v_num=0, train_loss=4.160]\n",
      "Epoch 0:   2%|▏         | 3/125 [01:49<1:14:21, 36.57s/it, v_num=0, train_loss=3.750]\n",
      "Epoch 0:   3%|▎         | 4/125 [02:33<1:17:35, 38.47s/it, v_num=0, train_loss=3.690]\n",
      "Epoch 0:   4%|▍         | 5/125 [03:08<1:15:35, 37.80s/it, v_num=0, train_loss=2.300]\n",
      "Epoch 0:   5%|▍         | 6/125 [03:48<1:15:30, 38.07s/it, v_num=0, train_loss=2.480]\n",
      "Epoch 0:   6%|▌         | 7/125 [04:22<1:13:44, 37.50s/it, v_num=0, train_loss=2.300]\n",
      "Epoch 0:   6%|▋         | 8/125 [05:03<1:14:02, 37.97s/it, v_num=0, train_loss=2.220]\n",
      "Epoch 0:   7%|▋         | 9/125 [05:37<1:12:27, 37.48s/it, v_num=0, train_loss=1.980]\n",
      "Epoch 0:   8%|▊         | 10/125 [06:19<1:12:39, 37.91s/it, v_num=0, train_loss=2.000]\n",
      "Epoch 0:   9%|▉         | 11/125 [06:52<1:11:14, 37.50s/it, v_num=0, train_loss=1.950]\n",
      "Epoch 0:  10%|▉         | 12/125 [07:33<1:11:06, 37.76s/it, v_num=0, train_loss=1.840]\n",
      "Epoch 0:  10%|█         | 13/125 [08:06<1:09:50, 37.42s/it, v_num=0, train_loss=1.970]\n",
      "Epoch 0:  11%|█         | 14/125 [08:46<1:09:31, 37.58s/it, v_num=0, train_loss=1.850]\n",
      "Epoch 0:  12%|█▏        | 15/125 [09:22<1:08:41, 37.47s/it, v_num=0, train_loss=1.730]\n",
      "Epoch 0:  13%|█▎        | 16/125 [10:01<1:08:15, 37.58s/it, v_num=0, train_loss=1.960]\n",
      "Epoch 0:  14%|█▎        | 17/125 [10:35<1:07:17, 37.38s/it, v_num=0, train_loss=1.720]\n",
      "Epoch 0:  14%|█▍        | 18/125 [11:15<1:06:58, 37.55s/it, v_num=0, train_loss=1.790]\n",
      "Epoch 0:  15%|█▌        | 19/125 [11:48<1:05:54, 37.30s/it, v_num=0, train_loss=1.730]\n",
      "Epoch 0:  16%|█▌        | 20/125 [12:29<1:05:34, 37.47s/it, v_num=0, train_loss=1.660]\n",
      "Epoch 0:  17%|█▋        | 21/125 [13:03<1:04:42, 37.33s/it, v_num=0, train_loss=1.780]\n",
      "Epoch 0:  18%|█▊        | 22/125 [13:46<1:04:28, 37.56s/it, v_num=0, train_loss=1.880]\n",
      "Epoch 0:  18%|█▊        | 23/125 [14:19<1:03:31, 37.37s/it, v_num=0, train_loss=1.700]\n",
      "Epoch 0:  19%|█▉        | 24/125 [15:00<1:03:09, 37.52s/it, v_num=0, train_loss=1.700]\n",
      "Epoch 0:  20%|██        | 25/125 [15:34<1:02:18, 37.39s/it, v_num=0, train_loss=1.600]\n",
      "Epoch 0:  21%|██        | 26/125 [16:15<1:01:55, 37.53s/it, v_num=0, train_loss=1.700]\n",
      "Epoch 0:  22%|██▏       | 27/125 [16:48<1:01:02, 37.37s/it, v_num=0, train_loss=1.660]\n",
      "Epoch 0:  22%|██▏       | 28/125 [17:32<1:00:47, 37.60s/it, v_num=0, train_loss=1.660]\n",
      "Epoch 0:  23%|██▎       | 29/125 [18:07<59:59, 37.49s/it, v_num=0, train_loss=1.810]  \n",
      "Epoch 0:  24%|██▍       | 30/125 [18:53<59:48, 37.77s/it, v_num=0, train_loss=1.650]\n",
      "Epoch 0:  25%|██▍       | 31/125 [19:26<58:58, 37.64s/it, v_num=0, train_loss=1.540]\n",
      "Epoch 0:  26%|██▌       | 32/125 [20:05<58:23, 37.67s/it, v_num=0, train_loss=1.580]\n",
      "Epoch 0:  26%|██▋       | 33/125 [20:39<57:35, 37.56s/it, v_num=0, train_loss=1.600]\n",
      "Epoch 0:  27%|██▋       | 34/125 [21:19<57:03, 37.62s/it, v_num=0, train_loss=1.700]\n",
      "Epoch 0:  28%|██▊       | 35/125 [21:52<56:16, 37.51s/it, v_num=0, train_loss=1.560]\n",
      "Epoch 0:  29%|██▉       | 36/125 [22:35<55:51, 37.66s/it, v_num=0, train_loss=1.700]\n",
      "Epoch 0:  30%|██▉       | 37/125 [23:08<55:02, 37.53s/it, v_num=0, train_loss=1.750]\n",
      "Epoch 0:  30%|███       | 38/125 [23:51<54:36, 37.66s/it, v_num=0, train_loss=1.650]\n",
      "Epoch 0:  31%|███       | 39/125 [24:24<53:49, 37.55s/it, v_num=0, train_loss=1.540]\n",
      "Epoch 0:  32%|███▏      | 40/125 [25:06<53:20, 37.66s/it, v_num=0, train_loss=1.730]\n",
      "Epoch 0:  33%|███▎      | 41/125 [25:40<52:35, 37.57s/it, v_num=0, train_loss=1.620]\n",
      "Epoch 0:  34%|███▎      | 42/125 [26:19<52:00, 37.60s/it, v_num=0, train_loss=1.660]\n",
      "Epoch 0:  34%|███▍      | 43/125 [26:53<51:17, 37.53s/it, v_num=0, train_loss=1.690]\n",
      "Epoch 0:  35%|███▌      | 44/125 [27:34<50:46, 37.61s/it, v_num=0, train_loss=1.730]\n",
      "Epoch 0:  36%|███▌      | 45/125 [28:07<49:59, 37.49s/it, v_num=0, train_loss=1.610]\n",
      "Epoch 0:  37%|███▋      | 46/125 [28:46<49:25, 37.53s/it, v_num=0, train_loss=1.540]\n",
      "Epoch 0:  38%|███▊      | 47/125 [29:20<48:41, 37.46s/it, v_num=0, train_loss=1.520]\n",
      "Epoch 0:  38%|███▊      | 48/125 [29:59<48:06, 37.49s/it, v_num=0, train_loss=1.610]\n",
      "Epoch 0:  39%|███▉      | 49/125 [30:32<47:22, 37.40s/it, v_num=0, train_loss=1.680]\n",
      "Epoch 0:  40%|████      | 50/125 [31:14<46:51, 37.49s/it, v_num=0, train_loss=1.590]\n",
      "Epoch 0:  41%|████      | 51/125 [31:48<46:09, 37.42s/it, v_num=0, train_loss=1.560]\n",
      "Epoch 0:  42%|████▏     | 52/125 [32:29<45:36, 37.49s/it, v_num=0, train_loss=1.610]\n",
      "Epoch 0:  42%|████▏     | 53/125 [33:04<44:56, 37.45s/it, v_num=0, train_loss=1.460]\n",
      "Epoch 0:  43%|████▎     | 54/125 [33:44<44:22, 37.50s/it, v_num=0, train_loss=1.590]\n",
      "Epoch 0:  44%|████▍     | 55/125 [34:17<43:38, 37.40s/it, v_num=0, train_loss=1.660]\n",
      "Epoch 0:  45%|████▍     | 56/125 [34:55<43:01, 37.42s/it, v_num=0, train_loss=1.490]\n",
      "Epoch 0:  46%|████▌     | 57/125 [35:30<42:21, 37.38s/it, v_num=0, train_loss=1.520]\n",
      "Epoch 0:  46%|████▋     | 58/125 [36:11<41:47, 37.43s/it, v_num=0, train_loss=1.550]\n",
      "Epoch 0:  47%|████▋     | 59/125 [36:44<41:05, 37.36s/it, v_num=0, train_loss=1.530]\n",
      "Epoch 0:  48%|████▊     | 60/125 [37:26<40:33, 37.44s/it, v_num=0, train_loss=1.620]\n",
      "Epoch 0:  49%|████▉     | 61/125 [37:59<39:51, 37.36s/it, v_num=0, train_loss=1.550]\n",
      "Epoch 0:  50%|████▉     | 62/125 [38:39<39:16, 37.40s/it, v_num=0, train_loss=1.430]\n",
      "Epoch 0:  50%|█████     | 63/125 [39:11<38:34, 37.33s/it, v_num=0, train_loss=1.590]\n",
      "Epoch 0:  51%|█████     | 64/125 [39:51<37:59, 37.37s/it, v_num=0, train_loss=1.550]\n",
      "Epoch 0:  52%|█████▏    | 65/125 [40:25<37:18, 37.31s/it, v_num=0, train_loss=1.520]\n",
      "Epoch 0:  53%|█████▎    | 66/125 [41:05<36:44, 37.36s/it, v_num=0, train_loss=1.450]\n",
      "Epoch 0:  54%|█████▎    | 67/125 [41:44<36:08, 37.39s/it, v_num=0, train_loss=1.330]\n",
      "Epoch 0:  54%|█████▍    | 68/125 [42:31<35:38, 37.52s/it, v_num=0, train_loss=1.410]\n",
      "Epoch 0:  55%|█████▌    | 69/125 [43:06<34:59, 37.49s/it, v_num=0, train_loss=1.370]\n",
      "Epoch 0:  56%|█████▌    | 70/125 [43:47<34:24, 37.53s/it, v_num=0, train_loss=1.450]\n",
      "Epoch 0:  57%|█████▋    | 71/125 [44:19<33:42, 37.46s/it, v_num=0, train_loss=1.450]\n",
      "Epoch 0:  58%|█████▊    | 72/125 [44:58<33:06, 37.48s/it, v_num=0, train_loss=1.550]\n",
      "Epoch 0:  58%|█████▊    | 73/125 [45:31<32:25, 37.42s/it, v_num=0, train_loss=1.570]\n",
      "Epoch 0:  59%|█████▉    | 74/125 [46:10<31:49, 37.44s/it, v_num=0, train_loss=1.510]\n",
      "Epoch 0:  60%|██████    | 75/125 [46:43<31:08, 37.38s/it, v_num=0, train_loss=1.530]\n",
      "Epoch 0:  61%|██████    | 76/125 [47:23<30:33, 37.42s/it, v_num=0, train_loss=1.480]\n",
      "Epoch 0:  62%|██████▏   | 77/125 [47:58<29:54, 37.38s/it, v_num=0, train_loss=1.450]\n",
      "Epoch 0:  62%|██████▏   | 78/125 [48:37<29:18, 37.40s/it, v_num=0, train_loss=1.350]\n",
      "Epoch 0:  63%|██████▎   | 79/125 [49:09<28:37, 37.34s/it, v_num=0, train_loss=1.500]\n",
      "Epoch 0:  64%|██████▍   | 80/125 [49:48<28:00, 37.35s/it, v_num=0, train_loss=1.480]\n",
      "Epoch 0:  65%|██████▍   | 81/125 [50:21<27:21, 37.30s/it, v_num=0, train_loss=1.590]\n",
      "Epoch 0:  66%|██████▌   | 82/125 [51:00<26:45, 37.33s/it, v_num=0, train_loss=1.270]\n",
      "Epoch 0:  66%|██████▋   | 83/125 [51:33<26:05, 37.27s/it, v_num=0, train_loss=1.360]\n",
      "Epoch 0:  67%|██████▋   | 84/125 [52:13<25:29, 37.31s/it, v_num=0, train_loss=1.450]\n",
      "Epoch 0:  68%|██████▊   | 85/125 [52:47<24:50, 37.26s/it, v_num=0, train_loss=1.380]\n",
      "Epoch 0:  69%|██████▉   | 86/125 [53:27<24:14, 37.30s/it, v_num=0, train_loss=1.520]\n",
      "Epoch 0:  70%|██████▉   | 87/125 [54:01<23:35, 37.26s/it, v_num=0, train_loss=1.410]\n",
      "Epoch 0:  70%|███████   | 88/125 [54:40<22:59, 37.28s/it, v_num=0, train_loss=1.560]\n",
      "Epoch 0:  71%|███████   | 89/125 [55:12<22:19, 37.22s/it, v_num=0, train_loss=1.410]\n",
      "Epoch 0:  72%|███████▏  | 90/125 [55:52<21:43, 37.25s/it, v_num=0, train_loss=1.560]\n",
      "Epoch 0:  73%|███████▎  | 91/125 [56:26<21:05, 37.21s/it, v_num=0, train_loss=1.490]\n",
      "Epoch 0:  74%|███████▎  | 92/125 [57:04<20:28, 37.22s/it, v_num=0, train_loss=1.430]\n",
      "Epoch 0:  74%|███████▍  | 93/125 [57:37<19:49, 37.17s/it, v_num=0, train_loss=1.430]\n",
      "Epoch 0:  75%|███████▌  | 94/125 [58:15<19:12, 37.19s/it, v_num=0, train_loss=1.460]\n",
      "Epoch 0:  76%|███████▌  | 95/125 [58:48<18:34, 37.14s/it, v_num=0, train_loss=1.510]\n",
      "Epoch 0:  77%|███████▋  | 96/125 [59:28<17:57, 37.17s/it, v_num=0, train_loss=1.450]\n",
      "Epoch 0:  78%|███████▊  | 97/125 [1:00:01<17:19, 37.13s/it, v_num=0, train_loss=1.410]\n",
      "Epoch 0:  78%|███████▊  | 98/125 [1:00:40<16:42, 37.15s/it, v_num=0, train_loss=1.450]\n",
      "Epoch 0:  79%|███████▉  | 99/125 [1:01:12<16:04, 37.10s/it, v_num=0, train_loss=1.370]\n",
      "Epoch 0:  80%|████████  | 100/125 [1:01:52<15:28, 37.13s/it, v_num=0, train_loss=1.430]\n",
      "Epoch 0:  81%|████████  | 101/125 [1:02:28<14:50, 37.11s/it, v_num=0, train_loss=1.480]\n",
      "Epoch 0:  82%|████████▏ | 102/125 [1:03:06<14:13, 37.12s/it, v_num=0, train_loss=1.420]\n",
      "Epoch 0:  82%|████████▏ | 103/125 [1:03:39<13:35, 37.09s/it, v_num=0, train_loss=1.400]\n",
      "Epoch 0:  83%|████████▎ | 104/125 [1:04:20<12:59, 37.12s/it, v_num=0, train_loss=1.410]\n",
      "Epoch 0:  84%|████████▍ | 105/125 [1:04:54<12:21, 37.09s/it, v_num=0, train_loss=1.580]\n",
      "Epoch 0:  85%|████████▍ | 106/125 [1:05:33<11:44, 37.10s/it, v_num=0, train_loss=1.460]\n",
      "Epoch 0:  86%|████████▌ | 107/125 [1:06:05<11:07, 37.06s/it, v_num=0, train_loss=1.420]\n",
      "Epoch 0:  86%|████████▋ | 108/125 [1:06:44<10:30, 37.08s/it, v_num=0, train_loss=1.450]\n",
      "Epoch 0:  87%|████████▋ | 109/125 [1:07:17<09:52, 37.04s/it, v_num=0, train_loss=1.470]\n",
      "Epoch 0:  88%|████████▊ | 110/125 [1:07:56<09:15, 37.06s/it, v_num=0, train_loss=1.500]\n",
      "Epoch 0:  89%|████████▉ | 111/125 [1:08:29<08:38, 37.02s/it, v_num=0, train_loss=1.370]\n",
      "Epoch 0:  90%|████████▉ | 112/125 [1:09:08<08:01, 37.04s/it, v_num=0, train_loss=1.450]\n",
      "Epoch 0:  90%|█████████ | 113/125 [1:09:40<07:23, 36.99s/it, v_num=0, train_loss=1.390]\n",
      "Epoch 0:  91%|█████████ | 114/125 [1:10:17<06:46, 37.00s/it, v_num=0, train_loss=1.450]\n",
      "Epoch 0:  92%|█████████▏| 115/125 [1:10:49<06:09, 36.95s/it, v_num=0, train_loss=1.380]\n",
      "Epoch 0:  93%|█████████▎| 116/125 [1:11:27<05:32, 36.96s/it, v_num=0, train_loss=1.520]\n",
      "Epoch 0:  94%|█████████▎| 117/125 [1:11:59<04:55, 36.92s/it, v_num=0, train_loss=1.490]\n",
      "Epoch 0:  94%|█████████▍| 118/125 [1:12:40<04:18, 36.95s/it, v_num=0, train_loss=1.390]\n",
      "Epoch 0:  95%|█████████▌| 119/125 [1:13:14<03:41, 36.93s/it, v_num=0, train_loss=1.500]\n",
      "Epoch 0:  96%|█████████▌| 120/125 [1:13:54<03:04, 36.96s/it, v_num=0, train_loss=1.520]\n",
      "Epoch 0:  97%|█████████▋| 121/125 [1:14:28<02:27, 36.93s/it, v_num=0, train_loss=1.410]\n",
      "Epoch 0:  98%|█████████▊| 122/125 [1:15:07<01:50, 36.95s/it, v_num=0, train_loss=1.540]\n",
      "Epoch 0:  98%|█████████▊| 123/125 [1:15:41<01:13, 36.92s/it, v_num=0, train_loss=1.440]\n",
      "Epoch 0:  99%|█████████▉| 124/125 [1:16:21<00:36, 36.95s/it, v_num=0, train_loss=1.300]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48011, ip=10.0.121.227)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48011, ip=10.0.121.227)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m No modifications detected for re-loaded extension module utils, skipping build step...\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m Using /home/ray/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m Loading extension module utils...\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 125/125 [1:16:55<00:00, 36.92s/it, v_num=0, train_loss=1.460]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6110, ip=10.0.120.86)\u001b[0m Uploading checkpoint files from worker rank 3 to cloud URI s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-test/vicuna-13b-finetune/LightningTrainer_d3e90_00000_0_2023-06-23_23-39-11/checkpoint_000000.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5970, ip=10.0.87.254)\u001b[0m   warnings.warn(\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=5845, ip=10.0.64.104)\u001b[0m Done uploading checkpoint files.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6364, ip=10.0.95.215)\u001b[0m Uploading checkpoint files from worker rank 1 to cloud URI s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-test/vicuna-13b-finetune/LightningTrainer_d3e90_00000_0_2023-06-23_23-39-11/checkpoint_000000.\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6053, ip=10.0.84.83)\u001b[0m Done uploading checkpoint files.\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 125/125 [1:17:48<00:00, 37.35s/it, v_num=0, train_loss=1.460]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LightningTrainer pid=5998, ip=10.0.84.83)\u001b[0m Uploading trial artifacts took 22.242 s, which may be a performance bottleneck. Consider saving fewer/smaller artifacts to the trial log directory, or disable artifact syncing with `SyncConfig(sync_artifacts=False)`.\n",
      "2023-06-24 00:59:43,285\tINFO tune.py:1148 -- Total run time: 4832.17 seconds (4789.95 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  metrics={'_report_on': 'train_epoch_end', 'train_loss': 1.4609375, 'epoch': 0, 'step': 62, 'should_checkpoint': True, 'done': True, 'trial_id': 'd3e90_00000', 'experiment_tag': '0'},\n",
       "  path='s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-test/vicuna-13b-finetune/LightningTrainer_d3e90_00000_0_2023-06-23_23-39-11',\n",
       "  checkpoint=LightningCheckpoint(uri=s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-test/vicuna-13b-finetune/LightningTrainer_d3e90_00000_0_2023-06-23_23-39-11/checkpoint_000000)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
