{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 16\n",
    "BATCH_SIZE_PER_WORKER = 8\n",
    "MODEL_NAME = \"/tmp/vicuna-13b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 17:19:54,231\tINFO worker.py:1426 -- Connecting to existing Ray cluster at address: 10.0.11.88:6379...\n",
      "2023-06-27 17:19:54,348\tINFO worker.py:1607 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-vzyh3916u4zwmf1es6fazmbrgm.i.anyscaleuserdata-staging.com \u001b[39m\u001b[22m\n",
      "2023-06-27 17:19:54,350\tINFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_78cceba9b6954f107800abddcdb68b76.zip' (0.15MiB) to Ray cluster...\n",
      "2023-06-27 17:19:54,351\tINFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_78cceba9b6954f107800abddcdb68b76.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(download_vicuna_13b pid=213755)\u001b[0m 0: Download finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "import os\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "def download_vicuna_13b(rank):\n",
    "    if not os.path.exists(MODEL_NAME):\n",
    "        print(f\"Rank {rank}: Downloading vicuna model...\")\n",
    "        os.system(\n",
    "            f\"aws s3 sync s3://large-dl-models-mirror/restricted/models--lmsys--vicuna-13b-delta-v1.1/main-safetensors/ {MODEL_NAME} >NUL 2>&1\"\n",
    "        )\n",
    "    print(f\"{rank}: Download finished!\")\n",
    "\n",
    "\n",
    "tasks = [download_vicuna_13b.remote(i) for i in range(NUM_WORKERS)]\n",
    "ray.get(tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELATION_TEMPLATE = {\n",
    "    0: \"[0]Cause-Effect({e1},{e2})\",\n",
    "    1: \"[1]Cause-Effect({e2},{e1})\",\n",
    "    2: \"[2]Component-Whole({e1},{e2})\",\n",
    "    3: \"[3]Component-Whole({e2},{e1})\",\n",
    "    4: \"[4]Content-Container({e1},{e2})\",\n",
    "    5: \"[5]Content-Container({e2},{e1})\",\n",
    "    6: \"[6]Entity-Destination({e1},{e2})\",\n",
    "    7: \"[7]Entity-Destination({e2},{e1})\",\n",
    "    8: \"[8]Entity-Origin({e1},{e2})\",\n",
    "    9: \"[9]Entity-Origin({e2},{e1})\",\n",
    "    10: \"[10]Instrument-Agency({e1},{e2})\",\n",
    "    11: \"[11]Instrument-Agency({e2},{e1})\",\n",
    "    12: \"[12]Member-Collection({e1},{e2})\",\n",
    "    13: \"[13]Member-Collection({e2},{e1})\",\n",
    "    14: \"[14]Message-Topic({e1},{e2})\",\n",
    "    15: \"[15]Message-Topic({e2},{e1})\",\n",
    "    16: \"[16]Product-Producer({e1},{e2})\",\n",
    "    17: \"[17]Product-Producer({e2},{e1})\",\n",
    "    18: \"[18]Unknown({e1},{e2})\",\n",
    "}\n",
    "\n",
    "PROMPT_TEMPLATE = \"{sentence}\\nIn the above sentence, the relationship between the two tagged entities is: {relation}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sem_eval_2010_task_8 (/home/ray/.cache/huggingface/datasets/sem_eval_2010_task_8/default/1.0.0/8545d1995bbbade386acf5c4e2bef5589d8387ae0a93356407dfb54cdb234416)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3492765b878451c8ee33eb5c97691b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 17:20:01,089\tWARNING dataset.py:249 -- \u001b[33mImportant: Ray Data requires schemas for all datasets in Ray 2.5. This means that standalone Python objects are no longer supported. In addition, the default batch format is fixed to NumPy. To revert to legacy behavior temporarily, set the environment variable RAY_DATA_STRICT_MODE=0 on all cluster processes.\n",
      "\n",
      "Learn more here: https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from ray.data.preprocessors import BatchMapper, Chain\n",
    "\n",
    "hf_dataset = load_dataset(\"sem_eval_2010_task_8\")\n",
    "hf_dataset[\"val\"] = hf_dataset[\"test\"].rename_column(\"sentence\", \"test_sentence\")\n",
    "ray_dataset = {\n",
    "    \"train\": ray.data.from_huggingface(hf_dataset[\"train\"]),\n",
    "    \"val\": ray.data.from_huggingface(hf_dataset[\"val\"]),\n",
    "}\n",
    "\n",
    "\n",
    "def fill_prompt(batch):\n",
    "    # Format train data\n",
    "    if \"sentence\" in batch:\n",
    "        # Extract two tagged entities\n",
    "        batch[\"e1\"] = batch[\"sentence\"].apply(\n",
    "            lambda x: re.search(r\"<e1>(.*?)</e1>\", x).group(1)\n",
    "        )\n",
    "        batch[\"e2\"] = batch[\"sentence\"].apply(\n",
    "            lambda x: re.search(r\"<e2>(.*?)</e2>\", x).group(1)\n",
    "        )\n",
    "        batch[\"input_sentence\"] = batch.apply(\n",
    "            lambda row: PROMPT_TEMPLATE.format(\n",
    "                sentence=row[\"sentence\"],\n",
    "                relation=RELATION_TEMPLATE[row[\"relation\"]].format(\n",
    "                    e1=row[\"e1\"], e2=row[\"e2\"]\n",
    "                ),\n",
    "            )\n",
    "            + \"</s>\",\n",
    "            axis=1,\n",
    "        )\n",
    "    # Format test data\n",
    "    else:\n",
    "        batch[\"input_sentence\"] = batch.apply(\n",
    "            lambda row: PROMPT_TEMPLATE.format(\n",
    "                sentence=row[\"test_sentence\"], relation=\"\"\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "    return batch[[\"input_sentence\", \"relation\"]]\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    ret = tokenizer(\n",
    "        list(batch[\"input_sentence\"]),\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    ret[\"labels\"] = ret[\"input_ids\"].copy()\n",
    "    ret[\"relation\"] = batch[\"relation\"]\n",
    "    return dict(ret)\n",
    "\n",
    "\n",
    "prompt_mapper = BatchMapper(fill_prompt, batch_format=\"pandas\")\n",
    "tokenize_mapper = BatchMapper(tokenize, batch_format=\"pandas\")\n",
    "preprocessor = Chain(prompt_mapper, tokenize_mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-27 17:20:05,117] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from deepspeed.ops.adam import DeepSpeedCPUAdam\n",
    "\n",
    "\n",
    "class ZeRO3Config:\n",
    "    def __init__(self, pl_module):\n",
    "        self.config = pl_module.trainer.strategy.config\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def is_zero3(self) -> bool:\n",
    "        return True\n",
    "\n",
    "\n",
    "def enable_transformers_pretrained_deepspeed_sharding(\n",
    "    pl_module: \"pl.LightningModule\",\n",
    ") -> None:\n",
    "    transformers.deepspeed._hf_deepspeed_config_weak_ref = ZeRO3Config(pl_module)\n",
    "\n",
    "\n",
    "class Vicuna13BModel(pl.LightningModule):\n",
    "    def __init__(self, inference=False):\n",
    "        super().__init__()\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "        if inference:\n",
    "            with init_empty_weights():\n",
    "                self.model_config = AutoConfig.from_pretrained(\n",
    "                    MODEL_NAME, trust_remote_code=True\n",
    "                )\n",
    "                self.model = AutoModelForCausalLM.from_config(\n",
    "                    self.model_config, trust_remote_code=True\n",
    "                )\n",
    "            self.model.tie_weights()\n",
    "\n",
    "        self.predictions = []\n",
    "        self.corrects = []\n",
    "\n",
    "    def setup(self, stage) -> None:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            enable_transformers_pretrained_deepspeed_sharding(self)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_NAME, trust_remote_code=True\n",
    "            )\n",
    "        if self.global_rank == 0:\n",
    "            print(\"DeepSpeed Configs: \", self.trainer.strategy.config)\n",
    "            print(\"Model Archetecture: \", self.model)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        outputs = self.model(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"],\n",
    "        )\n",
    "        return outputs.loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        torch.cuda.empty_cache()\n",
    "        loss = self.forward(batch)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output_tokens = self.model.generate(batch[\"input_ids\"], max_new_tokens=16)\n",
    "        output_sents = self.tokenizer.batch_decode(\n",
    "            output_tokens, skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        relations = batch[\"relation\"]\n",
    "        for rid, sent in zip(relations, output_sents):\n",
    "            correct = f\"[{int(rid)}]\" in sent\n",
    "            self.predictions.append({\"output\": sent, \"correct\": correct})\n",
    "            self.corrects.append(correct)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Dump predictions\n",
    "        with open(f\"/tmp/predictions.json\", \"w\") as fout:\n",
    "            for prediction in self.predictions:\n",
    "                fout.write(json.dumps(prediction) + \"\\n\")\n",
    "\n",
    "        # Report aggregated metrics\n",
    "        self.log(\"val_acc\", sum(self.corrects) / len(self.corrects), sync_dist=True)\n",
    "        self.corrects.clear()\n",
    "        self.predictions.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return DeepSpeedCPUAdam(self.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "\n",
    "# Create a customized progress bar for LightningTrainer\n",
    "class FalconProgressBar(TQDMProgressBar):\n",
    "    def __init__(self, num_iters_per_epoch, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_iters_per_epoch = num_iters_per_epoch\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, *_):\n",
    "        super().on_train_epoch_start(trainer, *_)\n",
    "        self.train_progress_bar.reset(self.num_iters_per_epoch)\n",
    "\n",
    "\n",
    "total_batches = ray_dataset[\"train\"].count()\n",
    "num_iters_per_epoch = total_batches // (NUM_WORKERS * BATCH_SIZE_PER_WORKER)\n",
    "progress_bar = FalconProgressBar(num_iters_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.lightning import LightningTrainer, LightningConfigBuilder\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "HIDDEN_SIZE = config.hidden_size\n",
    "\n",
    "# We are using default values from huggingface\n",
    "deepspeed_configs = {\n",
    "    \"zero_allow_untested_optimizer\": True,\n",
    "    \"bf16\": {\"enabled\": True},\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"offload_optimizer\": {\"device\": \"cpu\", \"pin_memory\": True},\n",
    "        \"overlap_comm\": True,\n",
    "        \"contiguous_gradients\": True,\n",
    "        \"reduce_bucket_size\": HIDDEN_SIZE * HIDDEN_SIZE,\n",
    "        \"stage3_prefetch_bucket_size\": 0.9 * HIDDEN_SIZE * HIDDEN_SIZE,\n",
    "        \"stage3_param_persistence_threshold\": 10 * HIDDEN_SIZE,\n",
    "        \"stage3_gather_16bit_weights_on_model_save\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "lightning_config = (\n",
    "    LightningConfigBuilder()\n",
    "    .module(cls=Vicuna13BModel)\n",
    "    .trainer(\n",
    "        max_epochs=1,\n",
    "        accelerator=\"gpu\",\n",
    "        precision=\"bf16-mixed\",\n",
    "        callbacks=[progress_bar],\n",
    "        accumulate_grad_batches=2,\n",
    "        limit_val_batches=1,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "    .strategy(name=\"deepspeed\", config=deepspeed_configs)\n",
    "    .checkpointing(save_top_k=0, save_weights_only=True, save_last=True)\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 17:20:06,229\tWARNING base_trainer.py:201 -- The `preprocessor` arg to Trainer is deprecated. Apply preprocessor transformations ahead of time by calling `preprocessor.transform(ds)`. Support for the preprocessor arg will be dropped in a future release.\n"
     ]
    }
   ],
   "source": [
    "from ray.air.config import CheckpointConfig, RunConfig, ScalingConfig\n",
    "\n",
    "trainer = LightningTrainer(\n",
    "    lightning_config=lightning_config,\n",
    "    run_config=RunConfig(\n",
    "        name=\"vicuna-13b-relation-extraction\",\n",
    "        storage_path=\"s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-test\",\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            _checkpoint_keep_all_ranks=True,\n",
    "            _checkpoint_upload_from_workers=True,\n",
    "        ),\n",
    "    ),\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=NUM_WORKERS,\n",
    "        use_gpu=True,\n",
    "        resources_per_worker={\"CPU\": 15, \"GPU\": 1},\n",
    "    ),\n",
    "    datasets=ray_dataset,\n",
    "    datasets_iter_config={\"batch_size\": BATCH_SIZE_PER_WORKER},\n",
    "    preprocessor=preprocessor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-06-27 18:09:19</td></tr>\n",
       "<tr><td>Running for: </td><td>00:49:13.10        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.4/62.1 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 241.0/256 CPUs, 16.0/16 GPUs (0.0/16.0 accelerator_type:A10G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  val_acc</th><th style=\"text-align: right;\">  epoch</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_88935_00000</td><td>TERMINATED</td><td>10.0.6.248:98259</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2708.96</td><td style=\"text-align: right;\">    0.691406</td><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">      0</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=98259, ip=10.0.6.248)\u001b[0m [2023-06-27 17:20:12,362] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[2m\u001b[36m(download_vicuna_13b pid=99058, ip=10.0.25.123)\u001b[0m 13: Download finished!\u001b[32m [repeated 15x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LightningTrainer pid=98259, ip=10.0.6.248)\u001b[0m The `preprocessor` arg to Trainer is deprecated. Apply preprocessor transformations ahead of time by calling `preprocessor.transform(ds)`. Support for the preprocessor arg will be dropped in a future release.\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=98259, ip=10.0.6.248)\u001b[0m \u001b[33mImportant: Ray Data requires schemas for all datasets in Ray 2.5. This means that standalone Python objects are no longer supported. In addition, the default batch format is fixed to NumPy. To revert to legacy behavior temporarily, set the environment variable RAY_DATA_STRICT_MODE=0 on all cluster processes.\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=98259, ip=10.0.6.248)\u001b[0m \n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=98259, ip=10.0.6.248)\u001b[0m Learn more here: https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode\u001b[0m\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=98259, ip=10.0.6.248)\u001b[0m Starting distributed worker processes: ['98320 (10.0.6.248)', '98862 (10.0.20.190)', '98215 (10.0.40.238)', '99040 (10.0.16.85)', '98679 (10.0.48.92)', '99256 (10.0.3.211)', '98834 (10.0.24.99)', '97932 (10.0.43.220)', '98197 (10.0.20.181)', '98250 (10.0.19.249)', '98788 (10.0.61.179)', '214121 (10.0.11.88)', '98999 (10.0.49.254)', '98856 (10.0.56.182)', '98990 (10.0.41.124)', '99151 (10.0.25.123)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m Setting up process group for: env:// [rank=0, world_size=16]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98259, ip=10.0.6.248) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98259, ip=10.0.6.248) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LightningTrainer pid=98259, ip=10.0.6.248)\u001b[0m Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(BatchMapper._transform_pandas)->MapBatches(BatchMapper._transform_pandas)] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=98259, ip=10.0.6.248)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=98259, ip=10.0.6.248)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98862, ip=10.0.20.190)\u001b[0m [2023-06-27 17:20:23,557] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m `Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98856, ip=10.0.56.182)\u001b[0m initializing deepspeed distributed: GLOBAL_RANK: 13, MEMBER: 14/16\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98990, ip=10.0.41.124)\u001b[0m Missing logger folder: /home/ray/ray_results/vicuna-13b-relation-extraction/LightningTrainer_88935_00000_0_2023-06-27_17-20-06/rank_all/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98856, ip=10.0.56.182)\u001b[0m [2023-06-27 17:20:24,595] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98999, ip=10.0.49.254)\u001b[0m initializing deepspeed distributed: GLOBAL_RANK: 12, MEMBER: 13/16\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98679, ip=10.0.48.92)\u001b[0m Missing logger folder: /home/ray/ray_results/vicuna-13b-relation-extraction/LightningTrainer_88935_00000_0_2023-06-27_17-20-06/rank_all/lightning_logs\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:  33%|███▎      | 1/3 [00:26<00:53, 26.60s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:  33%|███▎      | 1/3 [00:44<01:28, 44.08s/it]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [01:09<00:36, 36.08s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [01:09<00:36, 36.10s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [01:28<00:44, 44.07s/it]\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:37<00:00, 32.36s/it]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98250, ip=10.0.19.249)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:36<00:00, 32.30s/it]\u001b[32m [repeated 14x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m DeepSpeed Configs:  {'zero_allow_untested_optimizer': True, 'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'overlap_comm': True, 'contiguous_gradients': True, 'reduce_bucket_size': 26214400, 'stage3_prefetch_bucket_size': 23592960.0, 'stage3_param_persistence_threshold': 51200, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 2, 'train_micro_batch_size_per_gpu': 1, 'gradient_clipping': 0.0}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m Model Archetecture:  LlamaForCausalLM(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m   (model): LlamaModel(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m     (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m     (layers): ModuleList(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m       (0-39): 40 x LlamaDecoderLayer(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m         (self_attn): LlamaAttention(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m           (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m           (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m           (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m           (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m           (rotary_emb): LlamaRotaryEmbedding()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m         )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m         (mlp): LlamaMLP(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m           (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m           (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m           (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m           (act_fn): SiLUActivation()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m         )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m         (input_layernorm): LlamaRMSNorm()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m         (post_attention_layernorm): LlamaRMSNorm()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m       )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m     )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m     (norm): LlamaRMSNorm()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m   )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m   (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=214121)\u001b[0m [2023-06-27 17:20:23,794] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98999, ip=10.0.49.254)\u001b[0m [2023-06-27 17:20:24,601] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98856, ip=10.0.56.182)\u001b[0m Using /home/ray/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98999, ip=10.0.49.254)\u001b[0m Detected CUDA files, patching ldflags\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98999, ip=10.0.49.254)\u001b[0m Emitting ninja build file /home/ray/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98999, ip=10.0.49.254)\u001b[0m Building extension module cpu_adam...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98999, ip=10.0.49.254)\u001b[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98856, ip=10.0.56.182)\u001b[0m Loading extension module cpu_adam...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98856, ip=10.0.56.182)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:55<00:00, 38.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98856, ip=10.0.56.182)\u001b[0m ninja: no work to do.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98856, ip=10.0.56.182)\u001b[0m Time to load cpu_adam op: 2.3617641925811768 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=97932, ip=10.0.43.220)\u001b[0m Building extension module utils...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98862, ip=10.0.20.190)\u001b[0m Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98862, ip=10.0.20.190)\u001b[0m Time to load utils op: 0.07697224617004395 seconds\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m Parameter Offload: Total persistent parameters: 414720 in 81 params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m ninja: no work to do.\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98250, ip=10.0.19.249)\u001b[0m Time to load cpu_adam op: 2.3788340091705322 seconds\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98679, ip=10.0.48.92)\u001b[0m Time to load utils op: 0.0003402233123779297 seconds\u001b[32m [repeated 16x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98679, ip=10.0.48.92)\u001b[0m No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98679, ip=10.0.48.92)\u001b[0m Using /home/ray/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[32m [repeated 32x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98862, ip=10.0.20.190)\u001b[0m Detected CUDA files, patching ldflags\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m Emitting ninja build file /home/ray/.cache/torch_extensions/py310_cu118/utils/build.ninja...\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98862, ip=10.0.20.190)\u001b[0m Building extension module cpu_adam...\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98250, ip=10.0.19.249)\u001b[0m Loading extension module cpu_adam...\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m Building extension module utils...\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98679, ip=10.0.48.92)\u001b[0m Loading extension module utils...\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m   | Name  | Type             | Params | Params per Device\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m ---------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m 0 | model | LlamaForCausalLM | 13.0 B | 813 M            \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m ---------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m 13.0 B    Trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m 13.0 B    Total params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m 52,063.457Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/62 [00:00<?, ?it/s]48)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 1/62 [00:40<41:00, 40.34s/it, v_num=0, train_loss=4.560]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m Time to load utils op: 0.0008089542388916016 seconds\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:24:23,599] [WARNING] [stage3.py:1851:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   3%|▎         | 2/62 [01:20<40:14, 40.23s/it, v_num=0, train_loss=4.590]\n",
      "Epoch 0:   5%|▍         | 3/62 [01:56<38:01, 38.67s/it, v_num=0, train_loss=3.940]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:25:41,015] [WARNING] [stage3.py:1851:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   6%|▋         | 4/62 [02:37<38:09, 39.47s/it, v_num=0, train_loss=3.840]\n",
      "Epoch 0:   8%|▊         | 5/62 [03:12<36:33, 38.48s/it, v_num=0, train_loss=3.810]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:26:55,720] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  10%|▉         | 6/62 [03:52<36:10, 38.76s/it, v_num=0, train_loss=3.410]\n",
      "Epoch 0:  11%|█▏        | 7/62 [04:27<35:01, 38.20s/it, v_num=0, train_loss=1.560]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:28:10,208] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  13%|█▎        | 8/62 [05:07<34:32, 38.38s/it, v_num=0, train_loss=1.570]\n",
      "Epoch 0:  15%|█▍        | 9/62 [05:43<33:42, 38.15s/it, v_num=0, train_loss=1.200]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:29:26,893] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  16%|█▌        | 10/62 [06:23<33:15, 38.38s/it, v_num=0, train_loss=1.200]\n",
      "Epoch 0:  18%|█▊        | 11/62 [06:58<32:19, 38.02s/it, v_num=0, train_loss=1.160]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:30:43,328] [WARNING] [stage3.py:1851:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  19%|█▉        | 12/62 [07:40<31:57, 38.35s/it, v_num=0, train_loss=1.130]\n",
      "Epoch 0:  21%|██        | 13/62 [08:15<31:07, 38.11s/it, v_num=0, train_loss=1.090]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:31:58,566] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  23%|██▎       | 14/62 [08:55<30:35, 38.25s/it, v_num=0, train_loss=1.120]\n",
      "Epoch 0:  24%|██▍       | 15/62 [09:30<29:46, 38.01s/it, v_num=0, train_loss=1.040]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:33:13,052] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  26%|██▌       | 16/62 [10:09<29:13, 38.12s/it, v_num=0, train_loss=1.080]\n",
      "Epoch 0:  27%|██▋       | 17/62 [10:44<28:25, 37.90s/it, v_num=0, train_loss=1.010]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:34:27,496] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  29%|██▉       | 18/62 [11:24<27:52, 38.02s/it, v_num=0, train_loss=0.969]\n",
      "Epoch 0:  31%|███       | 19/62 [12:00<27:10, 37.92s/it, v_num=0, train_loss=0.906]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:35:46,575] [WARNING] [stage3.py:1851:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  32%|███▏      | 20/62 [12:43<26:43, 38.17s/it, v_num=0, train_loss=0.918]\n",
      "Epoch 0:  34%|███▍      | 21/62 [13:18<25:58, 38.02s/it, v_num=0, train_loss=0.867]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:37:02,684] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  35%|███▌      | 22/62 [13:59<25:26, 38.16s/it, v_num=0, train_loss=0.848]\n",
      "Epoch 0:  37%|███▋      | 23/62 [14:34<24:43, 38.03s/it, v_num=0, train_loss=0.812]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:38:21,588] [WARNING] [stage3.py:1851:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  39%|███▊      | 24/62 [15:18<24:14, 38.27s/it, v_num=0, train_loss=0.852]\n",
      "Epoch 0:  40%|████      | 25/62 [15:52<23:30, 38.11s/it, v_num=0, train_loss=0.832]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:39:38,204] [WARNING] [stage3.py:1851:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  42%|████▏     | 26/62 [16:35<22:57, 38.27s/it, v_num=0, train_loss=0.820]\n",
      "Epoch 0:  44%|████▎     | 27/62 [17:10<22:16, 38.17s/it, v_num=0, train_loss=0.844]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:40:56,490] [WARNING] [stage3.py:1851:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  45%|████▌     | 28/62 [17:53<21:43, 38.33s/it, v_num=0, train_loss=0.852]\n",
      "Epoch 0:  47%|████▋     | 29/62 [18:27<21:00, 38.19s/it, v_num=0, train_loss=0.789]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:42:11,152] [WARNING] [stage3.py:1851:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  48%|████▊     | 30/62 [19:08<20:24, 38.27s/it, v_num=0, train_loss=0.805]\n",
      "Epoch 0:  50%|█████     | 31/62 [19:43<19:43, 38.19s/it, v_num=0, train_loss=0.852]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:43:29,099] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  52%|█████▏    | 32/62 [20:25<19:09, 38.31s/it, v_num=0, train_loss=0.801]\n",
      "Epoch 0:  53%|█████▎    | 33/62 [21:01<18:28, 38.23s/it, v_num=0, train_loss=0.785]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:44:46,975] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  55%|█████▍    | 34/62 [21:43<17:53, 38.35s/it, v_num=0, train_loss=0.797]\n",
      "Epoch 0:  56%|█████▋    | 35/62 [22:18<17:12, 38.23s/it, v_num=0, train_loss=0.832]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:46:03,067] [WARNING] [stage3.py:1851:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  58%|█████▊    | 36/62 [22:59<16:36, 38.33s/it, v_num=0, train_loss=0.805]\n",
      "Epoch 0:  60%|█████▉    | 37/62 [23:33<15:55, 38.21s/it, v_num=0, train_loss=0.801]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:47:18,198] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  61%|██████▏   | 38/62 [24:15<15:18, 38.29s/it, v_num=0, train_loss=0.832]\n",
      "Epoch 0:  63%|██████▎   | 39/62 [24:50<14:38, 38.21s/it, v_num=0, train_loss=0.789]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:48:34,884] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  65%|██████▍   | 40/62 [25:31<14:02, 38.29s/it, v_num=0, train_loss=0.809]\n",
      "Epoch 0:  66%|██████▌   | 41/62 [26:05<13:22, 38.19s/it, v_num=0, train_loss=0.785]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:49:52,502] [WARNING] [stage3.py:1851:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  68%|██████▊   | 42/62 [26:49<12:46, 38.32s/it, v_num=0, train_loss=0.758]\n",
      "Epoch 0:  69%|██████▉   | 43/62 [27:24<12:06, 38.25s/it, v_num=0, train_loss=0.809]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:51:11,172] [WARNING] [stage3.py:1851:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  71%|███████   | 44/62 [28:08<11:30, 38.36s/it, v_num=0, train_loss=0.785]\n",
      "Epoch 0:  73%|███████▎  | 45/62 [28:43<10:51, 38.31s/it, v_num=0, train_loss=0.777]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:52:26,313] [WARNING] [stage3.py:1851:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  74%|███████▍  | 46/62 [29:23<10:13, 38.33s/it, v_num=0, train_loss=0.785]\n",
      "Epoch 0:  76%|███████▌  | 47/62 [29:58<09:33, 38.27s/it, v_num=0, train_loss=0.812]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:53:44,126] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  77%|███████▋  | 48/62 [30:40<08:56, 38.35s/it, v_num=0, train_loss=0.773]\n",
      "Epoch 0:  79%|███████▉  | 49/62 [31:16<08:17, 38.29s/it, v_num=0, train_loss=0.711]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:55:00,266] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  81%|████████  | 50/62 [31:57<07:40, 38.34s/it, v_num=0, train_loss=0.762]\n",
      "Epoch 0:  82%|████████▏ | 51/62 [32:31<07:00, 38.27s/it, v_num=0, train_loss=0.762]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:56:17,188] [WARNING] [stage3.py:1851:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  84%|████████▍ | 52/62 [33:14<06:23, 38.35s/it, v_num=0, train_loss=0.758]\n",
      "Epoch 0:  85%|████████▌ | 53/62 [33:48<05:44, 38.27s/it, v_num=0, train_loss=0.742]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:57:32,575] [WARNING] [stage3.py:1851:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  87%|████████▋ | 54/62 [34:29<05:06, 38.32s/it, v_num=0, train_loss=0.691]\n",
      "Epoch 0:  89%|████████▊ | 55/62 [35:05<04:27, 38.28s/it, v_num=0, train_loss=0.703]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 17:58:51,838] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  90%|█████████ | 56/62 [35:48<03:50, 38.37s/it, v_num=0, train_loss=0.715]\n",
      "Epoch 0:  92%|█████████▏| 57/62 [36:24<03:11, 38.33s/it, v_num=0, train_loss=0.676]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 18:00:09,376] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  94%|█████████▎| 58/62 [37:06<02:33, 38.38s/it, v_num=0, train_loss=0.723]\n",
      "Epoch 0:  95%|█████████▌| 59/62 [37:39<01:54, 38.30s/it, v_num=0, train_loss=0.707]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 18:01:24,171] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  97%|█████████▋| 60/62 [38:21<01:16, 38.35s/it, v_num=0, train_loss=0.730]\n",
      "Epoch 0:  98%|█████████▊| 61/62 [38:54<00:38, 38.27s/it, v_num=0, train_loss=0.688]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m [2023-06-27 18:02:38,159] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0: 100%|██████████| 62/62 [39:35<00:00, 38.31s/it, v_num=0, train_loss=0.680]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=214121)\u001b[0m Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(BatchMapper._transform_pandas)->MapBatches(BatchMapper._transform_pandas)] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=214121)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=214121)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m No modifications detected for re-loaded extension module utils, skipping build step...\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m Using /home/ray/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m Loading extension module utils...\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98788, ip=10.0.61.179) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98788, ip=10.0.61.179) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 63it [40:09, 38.25s/it, v_num=0, train_loss=0.691]{\"__magic_token__\": \"__ray_tqdm_magic_token__\", \"x\": 0, \"pos\": 1, \"desc\": \"- RandomizeBlockOrder\", \"total\": 1, \"ip\": \"10.0.6.248\", \"pid\": 98320, \"uuid\": \"5b4dc447ff674cdd9bc38a19390ebd1a\", \"closed\": false}, this may be due to logging too fast. This warning will not be printed again.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98320, ip=10.0.6.248) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98320, ip=10.0.6.248) - RandomizeBlockOrder: 0 active, 0 queued, 0.0 MiB objects, 0 output 1:   0%|      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98990, ip=10.0.41.124) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98990, ip=10.0.41.124) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98856, ip=10.0.56.182) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98856, ip=10.0.56.182) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=99151, ip=10.0.25.123) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=99151, ip=10.0.25.123) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=99256, ip=10.0.3.211) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=99256, ip=10.0.3.211) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98999, ip=10.0.49.254) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98999, ip=10.0.49.254) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98215, ip=10.0.40.238) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98215, ip=10.0.40.238) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98197, ip=10.0.20.181) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98197, ip=10.0.20.181) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98679, ip=10.0.48.92) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98679, ip=10.0.48.92) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98250, ip=10.0.19.249) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98250, ip=10.0.19.249) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=99040, ip=10.0.16.85) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=99040, ip=10.0.16.85) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98862, ip=10.0.20.190) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98862, ip=10.0.20.190) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98834, ip=10.0.24.99) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=98834, ip=10.0.24.99) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=214121) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6988a68a45b64102b14deb0a71c60d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=214121) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df475ad9de7d4581ad611490e3fd0af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=97932, ip=10.0.43.220) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1c5cfd858f40ea8dd024b45718c356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=97932, ip=10.0.43.220) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0, ip=10.0.6.248)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A0m \n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98679, ip=10.0.48.92)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98679, ip=10.0.48.92)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98834, ip=10.0.24.99)\u001b[0m Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(BatchMapper._transform_pandas)->MapBatches(BatchMapper._transform_pandas)] -> AllToAllOperator[RandomizeBlockOrder]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98834, ip=10.0.24.99)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98834, ip=10.0.24.99)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m \n",
      "Epoch 0: : 63it [40:38, 38.71s/it, v_num=0, train_loss=0.691]5.66s/it]\u001b[A\n",
      "Epoch 0: : 63it [40:38, 38.71s/it, v_num=0, train_loss=0.691]         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=99256, ip=10.0.3.211)\u001b[0m Uploading checkpoint files from worker rank 5 to cloud URI s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-test/vicuna-13b-relation-extraction/LightningTrainer_88935_00000_0_2023-06-27_17-20-06/checkpoint_000000.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98215, ip=10.0.40.238)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98215, ip=10.0.40.238)\u001b[0m   warnings.warn(\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=97932, ip=10.0.43.220)\u001b[0m Done uploading checkpoint files.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=214121)\u001b[0m Uploading checkpoint files from worker rank 11 to cloud URI s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-test/vicuna-13b-relation-extraction/LightningTrainer_88935_00000_0_2023-06-27_17-20-06/checkpoint_000000.\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=99256, ip=10.0.3.211)\u001b[0m Done uploading checkpoint files.\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=99151, ip=10.0.25.123)\u001b[0m Uploading checkpoint files from worker rank 15 to cloud URI s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-test/vicuna-13b-relation-extraction/LightningTrainer_88935_00000_0_2023-06-27_17-20-06/checkpoint_000000.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m Done uploading checkpoint files.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=99151, ip=10.0.25.123)\u001b[0m Done uploading checkpoint files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 63it [42:19, 40.30s/it, v_num=0, train_loss=0.691]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=98320, ip=10.0.6.248)\u001b[0m `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=98259, ip=10.0.6.248)\u001b[0m Uploading trial artifacts took 23.895 s, which may be a performance bottleneck. Consider saving fewer/smaller artifacts to the trial log directory, or disable artifact syncing with `SyncConfig(sync_artifacts=False)`.\n",
      "2023-06-27 18:09:19,429\tWARNING experiment_state.py:306 -- Syncing the experiment checkpoint to cloud took a long time with 71.02 seconds. This can be due to a large number of trials, large logfiles, or throttling from the remote storage provider for too frequent syncs. If your `CheckpointConfig.num_to_keep` is a low number, this can trigger frequent syncing, in which case you should increase it. \n",
      "2023-06-27 18:09:19,438\tINFO tune.py:1148 -- Total run time: 2953.18 seconds (2812.50 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "result = trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"output\": \"The most common <e1>audits</e1> were about <e2>waste</e2> and recycling.\\nIn the above sentence, the relationship between the two tagged entities is: \", \"correct\": false}\n",
      "{\"output\": \"The <e1>company</e1> fabricates plastic <e2>chairs</e2>.\\nIn the above sentence, the relationship between the two tagged entities is: \", \"correct\": false}\n",
      "{\"output\": \"The school <e1>master</e1> teaches the lesson with a <e2>stick</e2>.\\nIn the above sentence, the relationship between the two tagged entities is: \", \"correct\": false}\n",
      "{\"output\": \"The suspect dumped the dead <e1>body</e1> into a local <e2>reservoir</e2>.\\nIn the above sentence, the relationship between the two tagged entities is: \", \"correct\": false}\n",
      "{\"output\": \"Avian <e1>influenza</e1> is an infectious disease of birds caused by type A strains of the influenza <e2>virus</e2>.\\nIn the above sentence, the relationship between the two tagged entities is: \", \"correct\": false}\n",
      "{\"output\": \"The <e1>ear</e1> of the African <e2>elephant</e2> is significantly larger--measuring 183 cm by 114 cm in the bush elephant.\\nIn the above sentence, the relationship between the two tagged entities is: \", \"correct\": false}\n",
      "{\"output\": \"A child is told a <e1>lie</e1> for several years by their <e2>parents</e2> before he/she realizes that a Santa Claus does not exist.\\nIn the above sentence, the relationship between the two tagged entities is: \", \"correct\": false}\n",
      "{\"output\": \"Skype, a free software, allows a <e1>hookup</e1> of multiple computer <e2>users</e2> to join in an online conference call without incurring any telephone costs.\\nIn the above sentence, the relationship between the two tagged entities is: \", \"correct\": false}\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/prediction_5.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ray.train.lightning import LightningCheckpoint\n",
    "\n",
    "# ckpt = LightningCheckpoint.from_uri(\"s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-test/vicuna-13b-relation-extraction/LightningTrainer_22972_00000_0_2023-06-26_16-06-51/checkpoint_000000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 sync s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-test/vicuna-13b-relation-extraction/LightningTrainer_22972_00000_0_2023-06-26_16-06-51/checkpoint_000000 /tmp/vicuna-re-ckpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
