{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrequirements:\\nsentencepiece\\ntransformers\\ndeepspeed\\naccelerate\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "requirements:\n",
    "sentencepiece\n",
    "transformers\n",
    "deepspeed\n",
    "accelerate\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 16\n",
    "BATCH_SIZE_PER_WORKER = 8\n",
    "MODEL_NAME = \"lmsys/vicuna-7b-v1.3\"\n",
    "\n",
    "RELATION_TEMPLATE = {\n",
    "    0: \"Cause-Effect({e1},{e2})\",\n",
    "    1: \"Cause-Effect({e2},{e1})\",\n",
    "    2: \"Component-Whole({e1},{e2})\",\n",
    "    3: \"Component-Whole({e2},{e1})\",\n",
    "    4: \"Content-Container({e1},{e2})\",\n",
    "    5: \"Content-Container({e2},{e1})\",\n",
    "    6: \"Entity-Destination({e1},{e2})\",\n",
    "    7: \"Entity-Destination({e2},{e1})\",\n",
    "    8: \"Entity-Origin({e1},{e2})\",\n",
    "    9: \"Entity-Origin({e2},{e1})\",\n",
    "    10: \"Instrument-Agency({e1},{e2})\",\n",
    "    11: \"Instrument-Agency({e2},{e1})\",\n",
    "    12: \"Member-Collection({e1},{e2})\",\n",
    "    13: \"Member-Collection({e2},{e1})\",\n",
    "    14: \"Message-Topic({e1},{e2})\",\n",
    "    15: \"Message-Topic({e2},{e1})\",\n",
    "    16: \"Product-Producer({e1},{e2})\",\n",
    "    17: \"Product-Producer({e2},{e1})\",\n",
    "    18: \"Unknown({e1},{e2})\",\n",
    "}\n",
    "\n",
    "PROMPT_TEMPLATE = \"\\\"{sentence}\\\"\\n. From the above sentence, the relationship between entity e1 and e2 is: {relation}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "\n",
    "vicuna_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "vicuna_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sem_eval_2010_task_8 (/home/ray/.cache/huggingface/datasets/sem_eval_2010_task_8/default/1.0.0/8545d1995bbbade386acf5c4e2bef5589d8387ae0a93356407dfb54cdb234416)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54236975d385481694218f2d3ca1c09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Snapshotting files: 100%|██████████| 46/46 [00:00<00:00, 8300.19file/s]\n",
      "2023-06-30 15:15:51,482\tINFO worker.py:1426 -- Connecting to existing Ray cluster at address: 10.0.59.66:6379...\n",
      "2023-06-30 15:15:51,489\tINFO worker.py:1607 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-vzyh3916u4zwmf1es6fazmbrgm.i.anyscaleuserdata-staging.com \u001b[39m\u001b[22m\n",
      "2023-06-30 15:15:51,493\tINFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e0ff1e773cbc0c5bc85a192c42b8a628.zip' (0.57MiB) to Ray cluster...\n",
      "2023-06-30 15:15:51,494\tINFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e0ff1e773cbc0c5bc85a192c42b8a628.zip'.\n",
      "2023-06-30 15:15:54,425\tWARNING dataset.py:249 -- \u001b[33mImportant: Ray Data requires schemas for all datasets in Ray 2.5. This means that standalone Python objects are no longer supported. In addition, the default batch format is fixed to NumPy. To revert to legacy behavior temporarily, set the environment variable RAY_DATA_STRICT_MODE=0 on all cluster processes.\n",
      "\n",
      "Learn more here: https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ray\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from ray.data.preprocessors import BatchMapper, Chain\n",
    "\n",
    "hf_dataset = load_dataset(\"sem_eval_2010_task_8\")\n",
    "ray_dataset = ray.data.from_huggingface(hf_dataset[\"train\"])\n",
    "\n",
    "\n",
    "def fill_prompt(batch):\n",
    "    # Format train data\n",
    "    batch[\"e1\"] = batch[\"sentence\"].apply(\n",
    "        lambda x: re.search(r\"<e1>(.*?)</e1>\", x).group(1)\n",
    "    )\n",
    "    batch[\"e2\"] = batch[\"sentence\"].apply(\n",
    "        lambda x: re.search(r\"<e2>(.*?)</e2>\", x).group(1)\n",
    "    )\n",
    "    batch[\"input_sentence\"] = batch.apply(\n",
    "        lambda row: PROMPT_TEMPLATE.format(\n",
    "            sentence=row[\"sentence\"],\n",
    "            relation=RELATION_TEMPLATE[row[\"relation\"]].format(\n",
    "                e1=\"e1\", #row[\"e1\"],\n",
    "                e2=\"e2\"#row[\"e2\"]\n",
    "            ),\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    return batch[[\"input_sentence\"]]\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\", use_fast=False)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    ret = tokenizer(\n",
    "        list(batch[\"input_sentence\"]),\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    ret[\"labels\"] = ret[\"input_ids\"].copy()\n",
    "    return dict(ret)\n",
    "\n",
    "\n",
    "prompt_mapper = BatchMapper(fill_prompt, batch_format=\"pandas\")\n",
    "tokenize_mapper = BatchMapper(tokenize, batch_format=\"pandas\")\n",
    "preprocessor = Chain(prompt_mapper, tokenize_mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-30 15:15:55,034] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from deepspeed.ops.adam import DeepSpeedCPUAdam\n",
    "\n",
    "\n",
    "class ZeRO3Config:\n",
    "    def __init__(self, pl_module):\n",
    "        self.config = pl_module.trainer.strategy.config\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def is_zero3(self) -> bool:\n",
    "        return True\n",
    "\n",
    "\n",
    "def enable_transformers_pretrained_deepspeed_sharding(\n",
    "    pl_module: \"pl.LightningModule\",\n",
    ") -> None:\n",
    "    transformers.deepspeed._hf_deepspeed_config_weak_ref = ZeRO3Config(pl_module)\n",
    "\n",
    "\n",
    "class Vicuna13BModel(pl.LightningModule):\n",
    "    def __init__(self, inference=False):\n",
    "        super().__init__()\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\", use_fast=False)\n",
    "        if inference:\n",
    "            with init_empty_weights():\n",
    "                self.model_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "                self.model = AutoModelForCausalLM.from_config(self.model_config)\n",
    "            self.model.tie_weights()\n",
    "\n",
    "    def setup(self, stage) -> None:\n",
    "        if not hasattr(self, \"model\"):\n",
    "            enable_transformers_pretrained_deepspeed_sharding(self)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_NAME, trust_remote_code=True\n",
    "            )\n",
    "        if self.global_rank == 0:\n",
    "            print(\"DeepSpeed Configs: \", self.trainer.strategy.config)\n",
    "            print(\"Model Archetecture: \", self.model)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        outputs = self.model(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"],\n",
    "        )\n",
    "        return outputs.loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # torch.cuda.empty_cache()\n",
    "        loss = self.forward(batch)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return DeepSpeedCPUAdam(self.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "\n",
    "# Create a customized progress bar for LightningTrainer\n",
    "class VicunaProgressBar(TQDMProgressBar):\n",
    "    def __init__(self, num_iters_per_epoch, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_iters_per_epoch = num_iters_per_epoch\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, *_):\n",
    "        super().on_train_epoch_start(trainer, *_)\n",
    "        self.train_progress_bar.reset(self.num_iters_per_epoch)\n",
    "\n",
    "\n",
    "total_batches = ray_dataset.count()\n",
    "num_iters_per_epoch = total_batches // (NUM_WORKERS * BATCH_SIZE_PER_WORKER)\n",
    "progress_bar = VicunaProgressBar(num_iters_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.lightning import LightningTrainer, LightningConfigBuilder\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "HIDDEN_SIZE = config.hidden_size\n",
    "\n",
    "# We are using default values from huggingface\n",
    "deepspeed_configs = {\n",
    "    \"zero_allow_untested_optimizer\": True,\n",
    "    \"bf16\": {\"enabled\": True},\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"offload_optimizer\": {\"device\": \"cpu\", \"pin_memory\": True},\n",
    "        \"overlap_comm\": True,\n",
    "        \"contiguous_gradients\": True,\n",
    "        \"reduce_bucket_size\": HIDDEN_SIZE * HIDDEN_SIZE,\n",
    "        \"stage3_prefetch_bucket_size\": 0.9 * HIDDEN_SIZE * HIDDEN_SIZE,\n",
    "        \"stage3_param_persistence_threshold\": 10 * HIDDEN_SIZE,\n",
    "    },\n",
    "}\n",
    "\n",
    "lightning_config = (\n",
    "    LightningConfigBuilder()\n",
    "    .module(cls=Vicuna13BModel)\n",
    "    .trainer(\n",
    "        max_epochs=1,\n",
    "        accelerator=\"gpu\",\n",
    "        precision=\"bf16-mixed\",\n",
    "        callbacks=[progress_bar],\n",
    "        accumulate_grad_batches=2,\n",
    "        limit_val_batches=1,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "    .strategy(name=\"deepspeed\", config=deepspeed_configs)\n",
    "    .checkpointing(save_top_k=0, save_weights_only=True, save_last=True)\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 15:15:56,501\tWARNING base_trainer.py:201 -- The `preprocessor` arg to Trainer is deprecated. Apply preprocessor transformations ahead of time by calling `preprocessor.transform(ds)`. Support for the preprocessor arg will be dropped in a future release.\n"
     ]
    }
   ],
   "source": [
    "from ray.air.config import CheckpointConfig, RunConfig, ScalingConfig\n",
    "\n",
    "trainer = LightningTrainer(\n",
    "    lightning_config=lightning_config,\n",
    "    run_config=RunConfig(\n",
    "        name=\"vicuna-13b-relation-extraction\",\n",
    "        storage_path=\"s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test\",\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            _checkpoint_keep_all_ranks=True,\n",
    "            _checkpoint_upload_from_workers=True,\n",
    "        ),\n",
    "    ),\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=NUM_WORKERS,\n",
    "        use_gpu=True,\n",
    "        resources_per_worker={\"CPU\": 15, \"GPU\": 1},\n",
    "    ),\n",
    "    datasets={\"train\": ray_dataset},\n",
    "    datasets_iter_config={\"batch_size\": BATCH_SIZE_PER_WORKER},\n",
    "    preprocessor=preprocessor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-06-30 15:36:40</td></tr>\n",
       "<tr><td>Running for: </td><td>00:20:44.24        </td></tr>\n",
       "<tr><td>Memory:      </td><td>16.0/249.1 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 241.0/304 CPUs, 16.0/16 GPUs (0.0/16.0 accelerator_type:A10G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">  step</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_af77e_00000</td><td>TERMINATED</td><td>10.0.59.66:146292</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1215.48</td><td style=\"text-align: right;\">    0.800781</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">    31</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=146292)\u001b[0m [2023-06-30 15:16:02,052] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LightningTrainer pid=146292)\u001b[0m The `preprocessor` arg to Trainer is deprecated. Apply preprocessor transformations ahead of time by calling `preprocessor.transform(ds)`. Support for the preprocessor arg will be dropped in a future release.\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=146292)\u001b[0m \u001b[33mImportant: Ray Data requires schemas for all datasets in Ray 2.5. This means that standalone Python objects are no longer supported. In addition, the default batch format is fixed to NumPy. To revert to legacy behavior temporarily, set the environment variable RAY_DATA_STRICT_MODE=0 on all cluster processes.\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=146292)\u001b[0m \n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=146292)\u001b[0m Learn more here: https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode\u001b[0m\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=146292)\u001b[0m Starting distributed worker processes: ['146456 (10.0.59.66)', '77983 (10.0.42.136)', '77725 (10.0.12.187)', '78274 (10.0.42.159)', '78357 (10.0.16.61)', '77971 (10.0.39.223)', '78320 (10.0.24.147)', '78053 (10.0.42.191)', '79012 (10.0.33.57)', '78093 (10.0.54.109)', '77844 (10.0.24.253)', '77918 (10.0.50.243)', '78070 (10.0.32.230)', '77674 (10.0.50.184)', '78843 (10.0.34.160)', '78279 (10.0.26.213)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m Setting up process group for: env:// [rank=0, world_size=16]\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=146292)\u001b[0m Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(BatchMapper._transform_pandas)->MapBatches(BatchMapper._transform_pandas)] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=146292)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=146292)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d3b3dc8cec4965927e762ac503be32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=146292) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db0c62fcb214c649b1a51fadcf0d9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=146292) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77983, ip=10.0.42.136)\u001b[0m [2023-06-30 15:16:15,404] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m `Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77844, ip=10.0.24.253)\u001b[0m initializing deepspeed distributed: GLOBAL_RANK: 10, MEMBER: 11/16\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77844, ip=10.0.24.253)\u001b[0m Missing logger folder: /home/ray/ray_results/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/rank_all/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77844, ip=10.0.24.253)\u001b[0m [2023-06-30 15:16:16,281] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77725, ip=10.0.12.187)\u001b[0m initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/16\u001b[32m [repeated 15x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=78093, ip=10.0.54.109)\u001b[0m Missing logger folder: /home/ray/ray_results/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/rank_all/lightning_logs\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:22<00:22, 22.40s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.19s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:26<00:26, 26.31s/it]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=78279, ip=10.0.26.213)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m DeepSpeed Configs:  {'zero_allow_untested_optimizer': True, 'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'overlap_comm': True, 'contiguous_gradients': True, 'reduce_bucket_size': 16777216, 'stage3_prefetch_bucket_size': 15099494.4, 'stage3_param_persistence_threshold': 40960}, 'gradient_accumulation_steps': 2, 'train_micro_batch_size_per_gpu': 1, 'gradient_clipping': 0.0}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m Model Archetecture:  LlamaForCausalLM(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m   (model): LlamaModel(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m     (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m     (layers): ModuleList(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m       (0-31): 32 x LlamaDecoderLayer(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m         (self_attn): LlamaAttention(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m           (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m           (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m           (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m           (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m           (rotary_emb): LlamaRotaryEmbedding()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m         )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m         (mlp): LlamaMLP(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m           (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m           (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m           (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m           (act_fn): SiLUActivation()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m         )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m         (input_layernorm): LlamaRMSNorm()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m         (post_attention_layernorm): LlamaRMSNorm()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m       )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m     )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m     (norm): LlamaRMSNorm()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m   )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m   (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m )\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77674, ip=10.0.50.184)\u001b[0m [2023-06-30 15:16:15,430] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77725, ip=10.0.12.187)\u001b[0m [2023-06-30 15:16:16,291] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77844, ip=10.0.24.253)\u001b[0m Using /home/ray/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 16.15s/it]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=79012, ip=10.0.33.57)\u001b[0m Detected CUDA files, patching ldflags\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=79012, ip=10.0.33.57)\u001b[0m Emitting ninja build file /home/ray/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=79012, ip=10.0.33.57)\u001b[0m Building extension module cpu_adam...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=79012, ip=10.0.33.57)\u001b[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77844, ip=10.0.24.253)\u001b[0m Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77844, ip=10.0.24.253)\u001b[0m ninja: no work to do.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77844, ip=10.0.24.253)\u001b[0m Time to load cpu_adam op: 2.334331512451172 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=78320, ip=10.0.24.147)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=78274, ip=10.0.42.159)\u001b[0m Building extension module utils...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77674, ip=10.0.50.184)\u001b[0m Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77674, ip=10.0.50.184)\u001b[0m Time to load utils op: 0.07638359069824219 seconds\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m Parameter Offload: Total persistent parameters: 266240 in 65 params\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77725, ip=10.0.12.187)\u001b[0m No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77725, ip=10.0.12.187)\u001b[0m Using /home/ray/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[32m [repeated 32x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=78843, ip=10.0.34.160)\u001b[0m Detected CUDA files, patching ldflags\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m Emitting ninja build file /home/ray/.cache/torch_extensions/py310_cu118/utils/build.ninja...\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=78843, ip=10.0.34.160)\u001b[0m Building extension module cpu_adam...\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=78843, ip=10.0.34.160)\u001b[0m Loading extension module cpu_adam...\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m Building extension module utils...\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77725, ip=10.0.12.187)\u001b[0m Loading extension module utils...\u001b[32m [repeated 16x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m ninja: no work to do.\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=78843, ip=10.0.34.160)\u001b[0m Time to load cpu_adam op: 2.3675966262817383 seconds\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77725, ip=10.0.12.187)\u001b[0m Time to load utils op: 0.0006890296936035156 seconds\u001b[32m [repeated 16x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m   | Name  | Type             | Params | Params per Device\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m ---------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m 0 | model | LlamaForCausalLM | 6.7 B  | 421 M            \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m ---------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m 6.7 B     Trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m 6.7 B     Total params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m 26,953.662Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/62 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 1/62 [00:20<20:21, 20.03s/it, v_num=0, train_loss=6.250]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m Time to load utils op: 0.0003604888916015625 seconds\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Epoch 0:   3%|▎         | 2/62 [00:39<19:58, 19.97s/it, v_num=0, train_loss=6.280]\n",
      "Epoch 0:   5%|▍         | 3/62 [00:55<18:14, 18.55s/it, v_num=0, train_loss=5.120]\n",
      "Epoch 0:   6%|▋         | 4/62 [01:15<18:19, 18.96s/it, v_num=0, train_loss=5.060]\n",
      "Epoch 0:   8%|▊         | 5/62 [01:32<17:39, 18.58s/it, v_num=0, train_loss=2.670]\n",
      "Epoch 0:  10%|▉         | 6/62 [01:53<17:38, 18.90s/it, v_num=0, train_loss=2.700]\n",
      "Epoch 0:  11%|█▏        | 7/62 [02:10<17:05, 18.65s/it, v_num=0, train_loss=1.690]\n",
      "Epoch 0:  13%|█▎        | 8/62 [02:29<16:49, 18.69s/it, v_num=0, train_loss=1.730]\n",
      "Epoch 0:  15%|█▍        | 9/62 [02:45<16:14, 18.38s/it, v_num=0, train_loss=1.660]\n",
      "Epoch 0:  16%|█▌        | 10/62 [03:05<16:02, 18.51s/it, v_num=0, train_loss=1.620]\n",
      "Epoch 0:  18%|█▊        | 11/62 [03:20<15:31, 18.26s/it, v_num=0, train_loss=2.440]\n",
      "Epoch 0:  19%|█▉        | 12/62 [03:39<15:15, 18.30s/it, v_num=0, train_loss=2.300]\n",
      "Epoch 0:  21%|██        | 13/62 [03:55<14:45, 18.08s/it, v_num=0, train_loss=1.390]\n",
      "Epoch 0:  23%|██▎       | 14/62 [04:14<14:33, 18.20s/it, v_num=0, train_loss=1.420]\n",
      "Epoch 0:  24%|██▍       | 15/62 [04:31<14:11, 18.12s/it, v_num=0, train_loss=1.310]\n",
      "Epoch 0:  26%|██▌       | 16/62 [04:51<13:58, 18.23s/it, v_num=0, train_loss=1.330]\n",
      "Epoch 0:  27%|██▋       | 17/62 [05:07<13:34, 18.10s/it, v_num=0, train_loss=1.130]\n",
      "Epoch 0:  29%|██▉       | 18/62 [05:26<13:19, 18.16s/it, v_num=0, train_loss=1.090]\n",
      "Epoch 0:  31%|███       | 19/62 [05:42<12:55, 18.04s/it, v_num=0, train_loss=0.949]\n",
      "Epoch 0:  32%|███▏      | 20/62 [06:01<12:38, 18.07s/it, v_num=0, train_loss=0.957]\n",
      "Epoch 0:  34%|███▍      | 21/62 [06:17<12:17, 17.98s/it, v_num=0, train_loss=1.090]\n",
      "Epoch 0:  35%|███▌      | 22/62 [06:36<12:01, 18.03s/it, v_num=0, train_loss=1.050]\n",
      "Epoch 0:  37%|███▋      | 23/62 [06:52<11:39, 17.94s/it, v_num=0, train_loss=0.938]\n",
      "Epoch 0:  39%|███▊      | 24/62 [07:11<11:22, 17.96s/it, v_num=0, train_loss=0.996]\n",
      "Epoch 0:  40%|████      | 25/62 [07:27<11:02, 17.91s/it, v_num=0, train_loss=1.050]\n",
      "Epoch 0:  42%|████▏     | 26/62 [07:47<10:47, 17.99s/it, v_num=0, train_loss=1.020]\n",
      "Epoch 0:  44%|████▎     | 27/62 [08:04<10:27, 17.93s/it, v_num=0, train_loss=1.060]\n",
      "Epoch 0:  45%|████▌     | 28/62 [08:23<10:10, 17.97s/it, v_num=0, train_loss=1.050]\n",
      "Epoch 0:  47%|████▋     | 29/62 [08:38<09:50, 17.89s/it, v_num=0, train_loss=1.050]\n",
      "Epoch 0:  48%|████▊     | 30/62 [08:57<09:33, 17.92s/it, v_num=0, train_loss=1.100]\n",
      "Epoch 0:  50%|█████     | 31/62 [09:13<09:13, 17.85s/it, v_num=0, train_loss=1.100]\n",
      "Epoch 0:  52%|█████▏    | 32/62 [09:31<08:55, 17.86s/it, v_num=0, train_loss=1.040]\n",
      "Epoch 0:  53%|█████▎    | 33/62 [09:46<08:35, 17.78s/it, v_num=0, train_loss=1.010]\n",
      "Epoch 0:  55%|█████▍    | 34/62 [10:06<08:19, 17.82s/it, v_num=0, train_loss=1.020]\n",
      "Epoch 0:  56%|█████▋    | 35/62 [10:23<08:00, 17.81s/it, v_num=0, train_loss=1.040]\n",
      "Epoch 0:  58%|█████▊    | 36/62 [10:42<07:43, 17.84s/it, v_num=0, train_loss=1.010]\n",
      "Epoch 0:  60%|█████▉    | 37/62 [10:57<07:24, 17.77s/it, v_num=0, train_loss=1.010]\n",
      "Epoch 0:  61%|██████▏   | 38/62 [11:16<07:07, 17.81s/it, v_num=0, train_loss=1.030]\n",
      "Epoch 0:  63%|██████▎   | 39/62 [11:31<06:47, 17.73s/it, v_num=0, train_loss=0.984]\n",
      "Epoch 0:  65%|██████▍   | 40/62 [11:50<06:30, 17.76s/it, v_num=0, train_loss=0.992]\n",
      "Epoch 0:  66%|██████▌   | 41/62 [12:06<06:12, 17.72s/it, v_num=0, train_loss=0.961]\n",
      "Epoch 0:  68%|██████▊   | 42/62 [12:25<05:54, 17.75s/it, v_num=0, train_loss=0.934]\n",
      "Epoch 0:  69%|██████▉   | 43/62 [12:40<05:36, 17.69s/it, v_num=0, train_loss=0.969]\n",
      "Epoch 0:  71%|███████   | 44/62 [12:59<05:18, 17.71s/it, v_num=0, train_loss=0.938]\n",
      "Epoch 0:  73%|███████▎  | 45/62 [13:15<05:00, 17.67s/it, v_num=0, train_loss=0.934]\n",
      "Epoch 0:  74%|███████▍  | 46/62 [13:34<04:43, 17.71s/it, v_num=0, train_loss=0.953]\n",
      "Epoch 0:  76%|███████▌  | 47/62 [13:51<04:25, 17.69s/it, v_num=0, train_loss=0.965]\n",
      "Epoch 0:  77%|███████▋  | 48/62 [14:12<04:08, 17.75s/it, v_num=0, train_loss=0.914]\n",
      "Epoch 0:  79%|███████▉  | 49/62 [14:27<03:50, 17.71s/it, v_num=0, train_loss=0.852]\n",
      "Epoch 0:  81%|████████  | 50/62 [14:46<03:32, 17.73s/it, v_num=0, train_loss=0.914]\n",
      "Epoch 0:  82%|████████▏ | 51/62 [15:02<03:14, 17.70s/it, v_num=0, train_loss=0.898]\n",
      "Epoch 0:  84%|████████▍ | 52/62 [15:22<02:57, 17.74s/it, v_num=0, train_loss=0.914]\n",
      "Epoch 0:  85%|████████▌ | 53/62 [15:38<02:39, 17.70s/it, v_num=0, train_loss=0.879]\n",
      "Epoch 0:  87%|████████▋ | 54/62 [15:57<02:21, 17.74s/it, v_num=0, train_loss=0.828]\n",
      "Epoch 0:  89%|████████▊ | 55/62 [16:13<02:03, 17.69s/it, v_num=0, train_loss=0.844]\n",
      "Epoch 0:  90%|█████████ | 56/62 [16:31<01:46, 17.71s/it, v_num=0, train_loss=0.848]\n",
      "Epoch 0:  92%|█████████▏| 57/62 [16:47<01:28, 17.67s/it, v_num=0, train_loss=0.812]\n",
      "Epoch 0:  94%|█████████▎| 58/62 [17:06<01:10, 17.70s/it, v_num=0, train_loss=0.848]\n",
      "Epoch 0:  95%|█████████▌| 59/62 [17:22<00:53, 17.67s/it, v_num=0, train_loss=0.824]\n",
      "Epoch 0:  97%|█████████▋| 60/62 [17:42<00:35, 17.70s/it, v_num=0, train_loss=0.863]\n",
      "Epoch 0:  98%|█████████▊| 61/62 [17:57<00:17, 17.66s/it, v_num=0, train_loss=0.805]\n",
      "Epoch 0: 100%|██████████| 62/62 [18:17<00:00, 17.70s/it, v_num=0, train_loss=0.793]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=78093, ip=10.0.54.109)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=78093, ip=10.0.54.109)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m No modifications detected for re-loaded extension module utils, skipping build step...\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m Using /home/ray/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m Loading extension module utils...\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 63it [18:36, 17.72s/it, v_num=0, train_loss=0.801]                      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m Uploading checkpoint files from worker rank 0 to cloud URI s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77971, ip=10.0.39.223)\u001b[0m /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77971, ip=10.0.39.223)\u001b[0m   warnings.warn(\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=78070, ip=10.0.32.230)\u001b[0m Done uploading checkpoint files.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=78070, ip=10.0.32.230)\u001b[0m Uploading checkpoint files from worker rank 12 to cloud URI s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000.\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=78053, ip=10.0.42.191)\u001b[0m Done uploading checkpoint files.\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 63it [19:04, 18.17s/it, v_num=0, train_loss=0.801]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=146456)\u001b[0m `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=146292)\u001b[0m Uploading trial artifacts took 16.813 s, which may be a performance bottleneck. Consider saving fewer/smaller artifacts to the trial log directory, or disable artifact syncing with `SyncConfig(sync_artifacts=False)`.\n",
      "2023-06-30 15:36:40,906\tWARNING tune.py:1122 -- Trial Runner checkpointing failed: Sync process failed: /home/ray/ray_results/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/.lazy_checkpoint_marker\n",
      "2023-06-30 15:36:40,912\tINFO tune.py:1148 -- Total run time: 1244.32 seconds (1243.92 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "result = trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  metrics={'_report_on': 'train_epoch_end', 'train_loss': 0.80078125, 'epoch': 0, 'step': 31, 'should_checkpoint': True, 'done': True, 'trial_id': 'af77e_00000', 'experiment_tag': '0'},\n",
       "  path='s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56',\n",
       "  checkpoint=LightningCheckpoint(uri=s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!awsv2 configure set s3.max_concurrent_requests 32\n",
    "!awsv2 configure set default.s3.preferred_transfer_client crt\n",
    "!awsv2 configure set default.s3.target_bandwidth 100Gb/s\n",
    "!awsv2 configure set default.s3.multipart_chunksize 8MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.RANK_0.files to ../../../mnt/local_storage/checkpoint/.RANK_0.files\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.RANK_4.files to ../../../mnt/local_storage/checkpoint/.RANK_4.files\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.tune_metadata to ../../../mnt/local_storage/checkpoint/.tune_metadata\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/_metadata.meta.pkl to ../../../mnt/local_storage/checkpoint/_metadata.meta.pkl\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/zero_pp_rank_6_mp_rank_00_model_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/zero_pp_rank_6_mp_rank_00_model_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.RANK_3.files to ../../../mnt/local_storage/checkpoint/.RANK_3.files\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.RANK_14.files to ../../../mnt/local_storage/checkpoint/.RANK_14.files\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.RANK_12.files to ../../../mnt/local_storage/checkpoint/.RANK_12.files\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.RANK_13.files to ../../../mnt/local_storage/checkpoint/.RANK_13.files\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.RANK_5.files to ../../../mnt/local_storage/checkpoint/.RANK_5.files\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.RANK_9.files to ../../../mnt/local_storage/checkpoint/.RANK_9.files\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/_preprocessor to ../../../mnt/local_storage/checkpoint/_preprocessor\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.RANK_8.files to ../../../mnt/local_storage/checkpoint/.RANK_8.files\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.metadata.pkl to ../../../mnt/local_storage/checkpoint/.metadata.pkl\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/zero_pp_rank_0_mp_rank_00_model_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.RANK_11.files to ../../../mnt/local_storage/checkpoint/.RANK_11.files\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.RANK_2.files to ../../../mnt/local_storage/checkpoint/.RANK_2.files\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.RANK_7.files to ../../../mnt/local_storage/checkpoint/.RANK_7.files\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/_preprocessor.meta.pkl to ../../../mnt/local_storage/checkpoint/_preprocessor.meta.pkl\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/zero_pp_rank_11_mp_rank_00_model_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/zero_pp_rank_11_mp_rank_00_model_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/zero_pp_rank_2_mp_rank_00_model_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/zero_pp_rank_2_mp_rank_00_model_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/zero_pp_rank_5_mp_rank_00_model_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/zero_pp_rank_5_mp_rank_00_model_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.RANK_1.files to ../../../mnt/local_storage/checkpoint/.RANK_1.files\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.RANK_10.files to ../../../mnt/local_storage/checkpoint/.RANK_10.files\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.RANK_15.files to ../../../mnt/local_storage/checkpoint/.RANK_15.files\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.RANK_6.files to ../../../mnt/local_storage/checkpoint/.RANK_6.files\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/.is_checkpoint to ../../../mnt/local_storage/checkpoint/.is_checkpoint\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/zero_pp_rank_10_mp_rank_00_model_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/zero_pp_rank_10_mp_rank_00_model_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/zero_pp_rank_12_mp_rank_00_model_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/zero_pp_rank_12_mp_rank_00_model_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/zero_pp_rank_13_mp_rank_00_model_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/zero_pp_rank_13_mp_rank_00_model_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/zero_pp_rank_7_mp_rank_00_model_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/zero_pp_rank_7_mp_rank_00_model_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/zero_pp_rank_8_mp_rank_00_model_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/zero_pp_rank_8_mp_rank_00_model_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/zero_to_fp32.py to ../../../mnt/local_storage/checkpoint/model/zero_to_fp32.py\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/zero_pp_rank_15_mp_rank_00_model_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/zero_pp_rank_15_mp_rank_00_model_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/zero_pp_rank_1_mp_rank_00_model_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/zero_pp_rank_1_mp_rank_00_model_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/zero_pp_rank_9_mp_rank_00_model_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/zero_pp_rank_9_mp_rank_00_model_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/latest to ../../../mnt/local_storage/checkpoint/model/latest\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/zero_pp_rank_4_mp_rank_00_model_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/zero_pp_rank_4_mp_rank_00_model_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/zero_pp_rank_3_mp_rank_00_model_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/zero_pp_rank_3_mp_rank_00_model_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/zero_pp_rank_14_mp_rank_00_model_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/zero_pp_rank_14_mp_rank_00_model_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/bf16_zero_pp_rank_10_mp_rank_00_optim_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/bf16_zero_pp_rank_10_mp_rank_00_optim_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/bf16_zero_pp_rank_11_mp_rank_00_optim_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/bf16_zero_pp_rank_11_mp_rank_00_optim_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/bf16_zero_pp_rank_12_mp_rank_00_optim_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/bf16_zero_pp_rank_12_mp_rank_00_optim_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/bf16_zero_pp_rank_13_mp_rank_00_optim_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/bf16_zero_pp_rank_13_mp_rank_00_optim_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/bf16_zero_pp_rank_14_mp_rank_00_optim_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/bf16_zero_pp_rank_14_mp_rank_00_optim_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/bf16_zero_pp_rank_15_mp_rank_00_optim_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/bf16_zero_pp_rank_15_mp_rank_00_optim_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/bf16_zero_pp_rank_8_mp_rank_00_optim_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/bf16_zero_pp_rank_8_mp_rank_00_optim_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt\n",
      "download: s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_af77e_00000_0_2023-06-30_15-15-56/checkpoint_000000/model/checkpoint/bf16_zero_pp_rank_9_mp_rank_00_optim_states.pt to ../../../mnt/local_storage/checkpoint/model/checkpoint/bf16_zero_pp_rank_9_mp_rank_00_optim_states.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system(f\"awsv2 s3 sync {result.checkpoint.uri} /mnt/local_storage/checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ray\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import (\n",
    "    init_empty_weights,\n",
    "    infer_auto_device_map,\n",
    "    load_checkpoint_and_dispatch,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing zero checkpoint '/mnt/local_storage/checkpoint/model/checkpoint'\n",
      "Detected checkpoint of type zero stage 3, world_size: 16\n",
      "Parsing checkpoint created by deepspeed==0.9.4\n",
      "Reconstructed Trainable fp32 state dict with 291 params 6738415616 elements\n"
     ]
    }
   ],
   "source": [
    "from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n",
    "\n",
    "\n",
    "def extract_fp32_ckpt_from_zero(zero_ckpt_dir):\n",
    "    state_dict = get_fp32_state_dict_from_zero_checkpoint(zero_ckpt_dir)\n",
    "    vicuna_state_dict = {\n",
    "        k.replace(\"_forward_module.model.\", \"\"): v for k, v in state_dict.items()\n",
    "    }\n",
    "    torch.save(vicuna_state_dict, os.path.join(zero_ckpt_dir, \"full_model.pt\"))\n",
    "\n",
    "\n",
    "full_model_ckpt_path = \"/mnt/local_storage/checkpoint/model/full_model.pt\"\n",
    "extract_fp32_ckpt_from_zero(\"/mnt/local_storage/checkpoint/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a model on meta device\n",
    "with init_empty_weights():\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    meta_model = AutoModelForCausalLM.from_config(config)\n",
    "meta_model.tie_weights()\n",
    "\n",
    "# Define the device mapping\n",
    "device_map = infer_auto_device_map(\n",
    "    meta_model,\n",
    "    max_memory={0: \"15GB\", \"cpu\": \"60GB\"},\n",
    "    no_split_module_classes=[\"LlamaDecoderLayer\"],\n",
    ")\n",
    "\n",
    "# Load the model parameters\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    meta_model,\n",
    "    checkpoint=full_model_ckpt_path,\n",
    "    device_map=device_map,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    device_map=device_map,\n",
    "    tokenizer=AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\", use_fast=False),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Message-Topic({e1},{e2})\n",
      "Model Output: [{'generated_text': 'The most common <e1>audits</e1> were about <e2>waste</e2> and recycling.\\nIn the above sentence, the relationship between the two tagged entities is: 1. Entity-Origin(e1,e2)'}]\n",
      "Answer: Product-Producer({e2},{e1})\n",
      "Model Output: [{'generated_text': 'The <e1>company</e1> fabricates plastic <e2>chairs</e2>.\\nIn the above sentence, the relationship between the two tagged entities is: 1. Unknown(e1,e2)'}]\n",
      "Answer: Instrument-Agency({e2},{e1})\n",
      "Model Output: [{'generated_text': 'The school <e1>master</e1> teaches the lesson with a <e2>stick</e2>.\\nIn the above sentence, the relationship between the two tagged entities is: 1. Entity-Origin(e1,e2)'}]\n",
      "Answer: Entity-Destination({e1},{e2})\n",
      "Model Output: [{'generated_text': 'The suspect dumped the dead <e1>body</e1> into a local <e2>reservoir</e2>.\\nIn the above sentence, the relationship between the two tagged entities is: 1. Unknown(e1,e2)'}]\n",
      "Answer: Cause-Effect({e2},{e1})\n",
      "Model Output: [{'generated_text': 'Avian <e1>influenza</e1> is an infectious disease of birds caused by type A strains of the influenza <e2>virus</e2>.\\nIn the above sentence, the relationship between the two tagged entities is: 1. Cause-Effect(e2,e1)'}]\n",
      "Answer: Component-Whole({e1},{e2})\n",
      "Model Output: [{'generated_text': 'The <e1>ear</e1> of the African <e2>elephant</e2> is significantly larger--measuring 183 cm by 114 cm in the bush elephant.\\nIn the above sentence, the relationship between the two tagged entities is: 1. Entity-Origin(e1,e2)'}]\n",
      "Answer: Product-Producer({e1},{e2})\n",
      "Model Output: [{'generated_text': 'A child is told a <e1>lie</e1> for several years by their <e2>parents</e2> before he/she realizes that a Santa Claus does not exist.\\nIn the above sentence, the relationship between the two tagged entities is: 1. Entity-Origin(e1,e2)'}]\n",
      "Answer: Member-Collection({e2},{e1})\n",
      "Model Output: [{'generated_text': 'Skype, a free software, allows a <e1>hookup</e1> of multiple computer <e2>users</e2> to join in an online conference call without incurring any telephone costs.\\nIn the above sentence, the relationship between the two tagged entities is: 1. Entity-Origin(e1,e2)'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:1081: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Component-Whole({e1},{e2})\n",
      "Model Output: [{'generated_text': 'The disgusting scene was retaliation against her brother Philip who rents the <e1>room</e1> inside this apartment <e2>house</e2> on Lombard street.\\nIn the above sentence, the relationship between the two tagged entities is: 1. Unknown(e1,e2)'}]\n",
      "Answer: Message-Topic({e1},{e2})\n",
      "Model Output: [{'generated_text': 'This <e1>thesis</e1> defines the <e2>clinical characteristics</e2> of amyloid disease.\\nIn the above sentence, the relationship between the two tagged entities is: 1. Entity-Origin(e1,e2)'}]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    testcase = hf_dataset[\"test\"][i]\n",
    "    prompt = PROMPT_TEMPLATE.format(sentence=testcase[\"sentence\"], relation=\"\")\n",
    "    output = generator(prompt, max_new_tokens=20, do_sample=False)\n",
    "\n",
    "    print(\"Answer:\", RELATION_TEMPLATE[testcase[\"relation\"]])\n",
    "    print(\"Model Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testcase: {'sentence': 'The most common <e1>audits</e1> were about <e2>waste</e2> and recycling.', 'relation': 14}\n",
      "Output: [{'generated_text': \"This is my first time using the <e1>saw</e1> and I'm not sure\"}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Testcase:\", testcase)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
